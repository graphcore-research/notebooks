{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation using Mixture of Experts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) 2023 Graphcore Ltd.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook outlines a character-level text generation model in PyTorch, showing how, starting from a standard model definition, we can define a **mixture of experts** model. The notebook is written in PyTorch, and allows for the model to be run on both CPU and IPUs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with installing the required modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q numpy torch matplotlib\n",
    "%pip install -q git+https://github.com/graphcore-research/poptorch-experimental-addons@14886d2285c3e45b0eadf4d719dae87d5f28b109"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also download the dataset and model checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget -q -P /tmp/ https://graphcore-research-public.s3.eu-west-1.amazonaws.com/2023-moe-notebook/wikitext103.zip\n",
    "unzip /tmp/wikitext103.zip -d /tmp/dataset_cache/\n",
    "rm /tmp/wikitext103.zip\n",
    "wget -q -P /tmp/ https://graphcore-research-public.s3.eu-west-1.amazonaws.com/2023-moe-notebook/checkpoints.zip\n",
    "unzip /tmp/checkpoints.zip -d /tmp/\n",
    "rm /tmp/checkpoints.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import poptorch\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = Path(os.getenv(\"DATASET_DIR\"))\n",
    "checkpoint_dir = Path(os.getenv(\"CHECKPOINT_DIR\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we use the freely-available character-level `WikiText-103` dataset that can be downloaded from [here](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/). In order to load and batch the data, we can use `Dataset` class from `data.py`. The class requires a corresponding character-level vocabulary that we will use to convert between the characters and the corresponding integer indices. In this case, the vocabulary is pre-defined in `vocab.json` and it contains all 5,008 different characters appearing in `WikiText-103`. Internally, `Dataset` class uses the `CharVocab` class from `data.py` to provide the `str_to_ids` and `ids_to_str` methods for converting between text and an array of character ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import CharVocab\n",
    "\n",
    "vocab = CharVocab.from_path(dataset_dir / \"wikitext103_raw/vocab.json\")\n",
    "print(f\"Number of characters in the vocabulary: {len(vocab):,}\")\n",
    "\n",
    "some_text = \"Hello world\"\n",
    "char_ids = vocab.str_to_ids(some_text)\n",
    "\n",
    "print(f\"{some_text} -> {char_ids} -> {vocab.ids_to_str(char_ids)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining a `Dataset` object, we need to pass the text and the vocabulary (which will internally be converted to a `CharVocab` object). Let's load up the `validation` dataset to see how it works (it is smaller than the training data so will load more quickly).\n",
    "\n",
    "**Note**: When the dataset is loaded, the whole text is processed and converted to integer ids - this may take a bit of time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import Dataset\n",
    "\n",
    "vocab_path = dataset_dir / \"wikitext103_raw/vocab.json\"\n",
    "text_path = dataset_dir / \"wikitext103_raw/valid.txt\"\n",
    "\n",
    "ds = Dataset.from_path(vocab_path, text_path)\n",
    "\n",
    "print(f\"Number of characters in the dataset: {len(ds):,}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our `Dataset` object, we can call the `batch()` method in order to return an iterator that will sequentially generate batches of our training, validation, or test data. For the `batch` method, we need to pass the `batch_size` (number of sequences in a batch), `sequence_length` (number of characters per sequence), as well as `overlap_length` (number of characters that overlap between consecutive sequences). Note that this means that start indices of consecutive sequences will be `sequence_length - overlap_length` characters apart. In addition, we can pass a `seed` parameter to randomly generate the order of batched sequences - leaving it to `None` will instead generate sequences in the order of appearance within the dataset.\n",
    "\n",
    "Each batch will consist of an input/target pair of character id sequences (where for each input, the target is shifted one place to the left, i.e. corresponding to the next character in the sequence), as well as a mask tensor that masks out the first `overlap_length` number of characters in each sequence - we will use this during training in order to prevent the first `overlap_length` character predictions to contribute to the model loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "sequence_length = 256\n",
    "overlap_length = 32\n",
    "seed = 100\n",
    "\n",
    "batch = next(\n",
    "    ds.batch(\n",
    "        batch_size,\n",
    "        sequence_length,\n",
    "        overlap_length,\n",
    "        seed,\n",
    "    )\n",
    ")\n",
    "x = batch[\"x\"]\n",
    "y = batch[\"y\"]\n",
    "mask = batch[\"mask\"]\n",
    "\n",
    "print(f\"Shape of the input: {x.shape}\")\n",
    "print(f\"Shape of the output: {y.shape}\")\n",
    "\n",
    "seq_num = 1\n",
    "print(\"Example input sequence:\\n\", ds.vocab.ids_to_str(x[seq_num]))\n",
    "print(\"Example target sequence:\\n\", ds.vocab.ids_to_str(y[seq_num]))\n",
    "print(\"Sequence mask:\\n\", mask[seq_num])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining the character-level text-generation baseline model. In this example we will use a simplified transformer-style architecture where instead of a multi-head attention block we use a causal one-dimensional convolutional layer. We construct the model by alternating convolutional and FFN layers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to construct a *causal* convolution, we use a regular `nn.Conv1d` layer with `in_channels=hidden_size` and `out_channels=hidden_size`, with the setting `padding=kernel_size - 1` which ensures that the output at each position in the sequence only depends on the *previous* characters in the input sequence. As this leads to additional dummy elements at the end of each sequence, we cut the last `kernel_size - 1` characters in the `forward` method. Note that additional reshaping and axis permutations need to be done in the `forward` method to align with the expected `Conv1d` input order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Conv1d):\n",
    "    \"\"\"Conv1d layer with left padding to enforce causality\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        kernel_size: int,\n",
    "        num_groups: int,\n",
    "        dtype: torch.dtype,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=hidden_size,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size - 1,\n",
    "            groups=num_groups,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.transpose(-1, -2)\n",
    "        x = super().forward(x)[..., : -(self.kernel_size[0] - 1)]\n",
    "        return x.transpose(-1, -2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FFN layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the FFN layer, we will use the standard transformer structure of an up and down projection with a nonlinearity in between. For this it's useful to define helper functions for initialising the layer weights and biases tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weight_tensor(shape: Tuple[int, ...], dtype: torch.dtype) -> nn.Parameter:\n",
    "    w = nn.Parameter(torch.empty(shape, dtype=dtype))\n",
    "    xavier_std = np.sqrt(2 / (shape[-2] + shape[-1]))\n",
    "    return nn.init.normal_(w, mean=0, std=xavier_std)\n",
    "\n",
    "\n",
    "def create_bias_tensor(out_features: int, dtype: torch.dtype) -> nn.Parameter:\n",
    "    b = nn.Parameter(torch.empty(out_features, dtype=dtype))\n",
    "    return nn.init.zeros_(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \"\"\"Transformer-style FFN layer\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size: int, expand_factor: int, dtype: torch.dtype):\n",
    "        super().__init__()\n",
    "        self.hidden_size, self.expand_factor, self.dtype = (\n",
    "            hidden_size,\n",
    "            expand_factor,\n",
    "            dtype,\n",
    "        )\n",
    "        self.w1 = create_weight_tensor(\n",
    "            (hidden_size, hidden_size * expand_factor), dtype\n",
    "        )\n",
    "        self.w2 = create_weight_tensor(\n",
    "            (hidden_size * expand_factor, hidden_size), dtype\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return F.relu(x @ self.w1) @ self.w2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our convolutional and FFN layers, we can build the core layer of our model following the Transformer recipe: sequence of `CausalConv1d` and `FFN` layers with residual connections and layer normalizations between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoreLayer(nn.Module):\n",
    "    \"\"\"Core model layer consisting of Conv1D + normalization + FFN\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        expand_factor: int,\n",
    "        kernel_size: int,\n",
    "        num_groups: int,\n",
    "        dtype: torch.dtype,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ffn = FFN(\n",
    "            hidden_size=hidden_size, expand_factor=expand_factor, dtype=dtype\n",
    "        )\n",
    "        self.conv = CausalConv1d(\n",
    "            hidden_size=hidden_size,\n",
    "            kernel_size=kernel_size,\n",
    "            num_groups=num_groups,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_size, dtype=dtype)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_size, dtype=dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_conv = F.relu(self.conv(x))\n",
    "        x = self.layer_norm1(x_conv + x)\n",
    "        x_ffn = self.ffn(x)\n",
    "        return self.layer_norm2(x_ffn + x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full model will now consist of a sequence of `CoreLayer` layers, with an additional `nn.Embedding` for the input embeddings, and a linear output projection from `hidden_size` to `vocab_size`. We additionally define the `masked_cross_entropy_loss` method in order to ignore the loss contributions of the characters based on the previously generated `mask`.\n",
    "\n",
    "In order to ensure our model can run on both IPUs and CPU we need to calculate and return the loss function within our `forward` method, as well as to wrap it within `poptorch.identity_loss` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    \"\"\"Character-level language model\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        hidden_size: int,\n",
    "        expand_factor: int,\n",
    "        kernel_size: int,\n",
    "        num_groups: int,\n",
    "        num_layers: int,\n",
    "        dtype: torch.dtype,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size, dtype=dtype)\n",
    "        self.core_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.core_layers.append(\n",
    "                CoreLayer(\n",
    "                    hidden_size=hidden_size,\n",
    "                    expand_factor=expand_factor,\n",
    "                    kernel_size=kernel_size,\n",
    "                    num_groups=num_groups,\n",
    "                    dtype=dtype,\n",
    "                )\n",
    "            )\n",
    "        self.w_out = create_weight_tensor(shape=(hidden_size, vocab_size), dtype=dtype)\n",
    "        self.b_out = create_bias_tensor(out_features=vocab_size, dtype=dtype)\n",
    "\n",
    "    def masked_cross_entropy_loss(\n",
    "        self, logits: torch.Tensor, y: torch.Tensor, mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        batch_loss = F.cross_entropy(\n",
    "            logits.reshape((-1, self.vocab_size)),\n",
    "            y.reshape(-1),\n",
    "            reduction=\"none\",\n",
    "        )\n",
    "        return (batch_loss * mask.reshape(-1)).sum() / mask.sum()\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, mask: torch.Tensor = None, y: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        embed = self.embedding(x)\n",
    "        for layer in self.core_layers:\n",
    "            embed = layer(embed)\n",
    "        logits = embed @ self.w_out + self.b_out\n",
    "\n",
    "        if self.training:\n",
    "            loss = self.masked_cross_entropy_loss(logits, y, mask)\n",
    "            return logits, poptorch.identity_loss(loss, reduction=\"none\")\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model *(can be skipped)*\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined all modules of our model, we can build and train the model on the `WikiText-103` dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we use the `AdamW` optimiser with learning rate decay. The following code showcases a basic training loop with these settings, making sure that the model is wrapped within `poptorch.trainingModel` when executing on the IPU. Note that running on the CPU is very slow and is mainly provided as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
        "jupyter": {
            "source_hidden": true
        }
   },
   "outputs": [],
   "source": [
    "# # Load the training dataset\n",
    "\n",
    "# vocab_path = dataset_dir / \"wikitext103_raw/vocab.json\"\n",
    "# train_path = dataset_dir / \"wikitext103_raw/train.txt\"\n",
    "\n",
    "# train_ds = Dataset.from_path(vocab_path, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
        "jupyter": {
            "source_hidden": true
        }
   },
   "outputs": [],
   "source": [
    "# # Set random seed\n",
    "# seed = None\n",
    "# if seed is None:\n",
    "#     seed = np.random.SeedSequence().generate_state(1)[0]\n",
    "\n",
    "# # Model hyperparameters\n",
    "# model = CharModel(\n",
    "#     vocab_size=len(ds.vocab),\n",
    "#     hidden_size=128,\n",
    "#     expand_factor=4,\n",
    "#     kernel_size=7,\n",
    "#     num_groups=8,\n",
    "#     num_layers=8,\n",
    "#     dtype=torch.float32,\n",
    "# )\n",
    "\n",
    "# # Batch settings\n",
    "# batch_size = 32\n",
    "# sequence_length = 256\n",
    "# overlap_length = 32\n",
    "# batch_seed = 100\n",
    "\n",
    "\n",
    "# # Change this to switch between IPU and CPU execution\n",
    "# run_on_ipu = True\n",
    "# num_replicas = 4\n",
    "\n",
    "# # Training parameters\n",
    "# lr = 2e-3\n",
    "# lr_decay = 2**-16\n",
    "# num_steps = 2**15\n",
    "\n",
    "\n",
    "# # Set the optimizer\n",
    "# if run_on_ipu:\n",
    "#     opts = poptorch.Options()\n",
    "#     opts.replicationFactor(num_replicas)\n",
    "#     optimizer = poptorch.optim.AdamW(model.parameters(), lr=lr)\n",
    "#     poptorch_model = poptorch.trainingModel(model, opts, optimizer)\n",
    "# else:\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# # Training loop\n",
    "# model.train()\n",
    "# lr_decay_factor = 2**-lr_decay\n",
    "# training_loss = {}\n",
    "# for step, batch in zip(\n",
    "#     range(num_steps),\n",
    "#     train_ds.batch(batch_size, sequence_length, overlap_length, batch_seed),\n",
    "# ):\n",
    "#     # Apply a step\n",
    "#     if run_on_ipu:\n",
    "#         out, loss = poptorch_model(batch[\"x\"].int(), batch[\"mask\"], batch[\"y\"].int())\n",
    "#         loss = torch.mean(loss)  # average over IPUs if num_replicas > 1\n",
    "#     else:\n",
    "#         optimizer.zero_grad()\n",
    "#         out, loss = model(batch[\"x\"], batch[\"mask\"], batch[\"y\"])\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     training_loss[step] = loss\n",
    "\n",
    "#     # Print results\n",
    "#     if step % 2**8 == 0:\n",
    "#         print(f\"Step {step}:\")\n",
    "#         print(f\"Loss: {loss}\")\n",
    "#         print()\n",
    "\n",
    "#     # Learning rate schedule\n",
    "#     lr *= lr_decay_factor\n",
    "#     optimizer.param_groups[0][\"lr\"] = lr\n",
    "#     if run_on_ipu:\n",
    "#         poptorch_model.setOptimizer(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
        "jupyter": {
            "source_hidden": true
        }
   },
   "outputs": [],
   "source": [
    "# # Plot training loss\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(training_loss.values())\n",
    "# plt.xlabel(\"Step\")\n",
    "# plt.ylabel(\"Cross entropy loss\")\n",
    "# plt.title(\"Training loss\")\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating text\n",
    "\n",
    "Let's see our baseline model in action! We can load a checkpoint of the model and use it in inference (or alternatively use the model we trained in the previous section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_text(\n",
    "    model: CharModel, text: str, num_characters: int, temperature: float = 1.0\n",
    ") -> str:\n",
    "    print(text, end=\"\")\n",
    "    model.train(False)\n",
    "    ids = vocab.str_to_ids(text)\n",
    "\n",
    "    for _ in range(num_characters):\n",
    "        y = model(ids)\n",
    "        new_char = torch.multinomial(\n",
    "            F.softmax(y[-1] / temperature, dim=0), num_samples=1\n",
    "        )\n",
    "        ids = torch.cat((ids, torch.tensor([new_char])), dim=0)\n",
    "        print(vocab.ids_to_str(new_char), end=\"\", flush=True)\n",
    "    print()\n",
    "    return vocab.ids_to_str(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_from_checkpoint = CharModel(\n",
    "    vocab_size=len(ds.vocab),\n",
    "    hidden_size=128,\n",
    "    expand_factor=4,\n",
    "    kernel_size=7,\n",
    "    num_groups=8,\n",
    "    num_layers=8,\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "model_from_checkpoint.load_state_dict(torch.load(checkpoint_dir / \"dense.pt\"))\n",
    "\n",
    "out = sample_text(\n",
    "    model_from_checkpoint, \"Hello worl\", num_characters=300, temperature=0.4\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can sample from the model we trained in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
        "jupyter": {
            "source_hidden": true
        }
   },
   "outputs": [],
   "source": [
    "# # Sample from trained model\n",
    "# print(sample_text(model, \"Hello world\", num_characters=300, temperature=0.5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture of experts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now see how we can turn our regular *dense* model into a *mixture of experts* model that we can run on IPUs/CPU."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the standard mixture of experts setup, we take some of the FFN layers of the model and turn them into sparse MoE layers. This is done by instantiating multiple copies of the basic FFN layer with independent parameters (i.e. experts), and routing the inputs to different experts based on a trained scoring function. For the purposes of this notebook, we will demonstrate how to define a setup closely following the architecture presented in the [Switch Transformers](https://arxiv.org/abs/2101.03961) paper."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switch router"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first tackle the problem of assigning individual tokens (throughout this notebook, characters) to experts, assuming we have a batch of tokens of shape `(batch_size, hidden_size)` that we want to allocate to `num_experts` experts. The starting point is to calculate a score for each token-expert assignment pair, and use these scores to assign tokens to experts. The optimal assignment problem can therefore be stated as maximising the total score of the assignments, while keeping the constraint that each expert is assigned exactly `expert_capacity = batch_size / num_experts` tokens. As solving this optimisation problem is computationally expensive, the Switch Transformer uses a simplified approach: assign each token to its preferred expert based on the scores, and afterwards make sure that each expert receives the same number of tokens. This is done by randomly disregarding some tokens for the over-assigned experts, and padding with \"dummy\" tokens the inputs to under-assigned experts.\n",
    "\n",
    "In order to train the scoring function throughout training, when the token `i` is routed to the expert `j`, the output of the expert is multiplied by `score[i, j]` which tends to strengthen the \"successful\" token-expert assignments and weaken the \"unsuccessful\" ones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the scoring function, we will use a simple linear feed-forward layer producing the output of shape `(batch_size, num_experts)` and normalising so that the scores for each token sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scorer(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_experts: int, dtype: torch.dtype):\n",
    "        super().__init__()\n",
    "        self.w = create_weight_tensor((hidden_size, num_experts), dtype)\n",
    "        self.b = create_bias_tensor(num_experts, dtype)\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Return per-token expert scores (normalised so rows sum to 1)\"\"\"\n",
    "        assert len(x.shape) == 2\n",
    "\n",
    "        logits = x @ self.w + self.b\n",
    "        return F.softmax(logits, dim=1, dtype=self.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assigning tokens to experts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the `Scorer` module is the scores matrix of shape `(batch_size, num_experts)` where each row sums to 1. We now want to map this to a per-expert assignments tensor `expert_assignments` of shape `(num_experts, expert_capacity)`, so that `expert_assignment[i, :]` contains all token ids assigned to expert `i`.\n",
    "\n",
    "As the procedure might need to cut-off tokens when an expert is over-assigned, we will introduce an additional parameter `capacity_factor` which allows us to increase the per-expert capacity.\n",
    "\n",
    "Let's now write several helper functions in order to tackle this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate assignment matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on previously calculated scores, assign the `top-1` expert to each token and create a binary mask indicating the token-expert assignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expert_mask(scores: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Return expert_mask[i, j] = 1 if token i assigned to expert j, 0 otherwise\"\"\"\n",
    "    token_assignments = torch.argmax(scores, dim=1)\n",
    "    return F.one_hot(token_assignments, scores.shape[1])\n",
    "\n",
    "\n",
    "# 4 tokens, 4 experts\n",
    "scores = torch.tensor(\n",
    "    [\n",
    "        [0.1, 0.2, 0.5, 0.2],\n",
    "        [0, 0, 0.9, 0.1],\n",
    "        [0.4, 0.3, 0.15, 0.15],\n",
    "        [0.2, 0.2, 0.2, 0.4],\n",
    "    ]\n",
    ")\n",
    "\n",
    "expert_mask = get_expert_mask(scores)\n",
    "print(\"Assignment matrix based on scores:\")\n",
    "print(expert_mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign tokens to experts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are no guarantees the previously generated assignment matrix is balanced (i.e. that all *columns* sum to `expert_capacity`), we will now write a function that will ensure this property by only assigning the first `expert_capacity` tokens to an expert and ignoring the rest, and similarly, assigning the \"dummy\" token id `batch_size` to fill the under-assigned experts (note that the actual token ids span from `0` to `batch_size - 1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_expert_assignments(\n",
    "    expert_mask: torch.Tensor, capacity: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert token to expert assignment matrix (expert mask) to expert assignments\n",
    "    of shape (num_experts, expert_capacity), discarding tokens for oversubscribed\n",
    "    experts and padding undersubscribed experts\n",
    "    \"\"\"\n",
    "    expert_mask_cumsum = torch.cumsum(expert_mask, dim=0)\n",
    "    capacity_range = torch.arange(\n",
    "        capacity, dtype=expert_mask.dtype, device=expert_mask.device\n",
    "    )\n",
    "    le_capacity = expert_mask_cumsum[:, :, None] <= capacity_range\n",
    "    expert_assignments = le_capacity.sum(dim=0)\n",
    "    return expert_assignments\n",
    "\n",
    "\n",
    "print(\"Final expert assignments:\")\n",
    "print(mask_to_expert_assignments(expert_mask, capacity=1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example with four tokens and four experts, each expert has the capacity of one token. As no tokens are assigned to expert 1, it gets the dummy token (token 4). Further, since both tokens 0 and 1 were assigned to expert 2, the assignment proceeds in order and token 0 is assigned to the expert, while token 1 remains unassigned. Tokens 2 and 3 are assigned to their prefered experts as there is no competition. The reader is encouraged to see what happens when the expert capacity is increased."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load balancing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue with the current procedure is that it creates unbalanced token-expert assignments throughout training. If early in the training an expert receives more tokens, over time it will preferentially receive more tokens due to it being more trained. This can quickly lead to assignments collapsing to only a small subset of the experts, leading to many tokens being cut-off and the degradation in performance. In order to tackle this, we introduce a load balancing auxiliary loss that penalises unbalanced assignments.\n",
    "\n",
    "We calculate this loss by averaging the per-token expert scores *before assignment* over the batch dimension (`density_proxy`), as well as calculating the *actual* proportions of tokens received by each expert by averaging `expert_mask` over the batch dimension (`density`). The final loss is then defined as the dot product between the two vectors. In a perfectly balanced situation all experts receive `1 / num_experts` proportion of the tokens, in which case `density[i] = 1 / num_experts` and `density_proxy[i] = 1 / num_experts` for each expert `i`. Therefore, we multiply the dot product with `num_experts` in order to normalise the balancing loss to `1.0` when all experts are balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balancing_loss(scores: torch.Tensor, expert_mask: torch.Tensor = None):\n",
    "    if expert_mask is None:\n",
    "        expert_mask = get_expert_mask(scores)\n",
    "    density = expert_mask.float().mean(dim=0)\n",
    "    density_proxy = scores.mean(dim=0)\n",
    "    return torch.dot(density, density_proxy) * density.shape[0]\n",
    "\n",
    "\n",
    "scores_balanced = torch.tensor(\n",
    "    [\n",
    "        [0.7, 0.1, 0.1, 0.1],\n",
    "        [0.1, 0.1, 0.1, 0.7],\n",
    "        [0.1, 0.7, 0.1, 0.1],\n",
    "        [0.1, 0.1, 0.7, 0.1],\n",
    "    ]\n",
    ")\n",
    "scores_unbalanced = torch.tensor(\n",
    "    [\n",
    "        [0.7, 0.1, 0.1, 0.1],\n",
    "        [0.7, 0.1, 0.1, 0.1],\n",
    "        [0.1, 0.7, 0.1, 0.1],\n",
    "        [0.1, 0.1, 0.7, 0.1],\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Balancing loss of a balanced assignment:\")\n",
    "print(get_balancing_loss(scores_balanced))\n",
    "print(\"Balancing loss of an unbalanced assignment:\")\n",
    "print(get_balancing_loss(scores_unbalanced))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it together"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything we need to construct a Switch router that will take a batch of tokens and output per-expert token assignments. We'll make sure we shuffle the tokens before the procedures outlined above to ensure a fair cut-off of tokens (otherwise tokens appearing early in sequence would have an advantage!)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shuffle tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_tokens(\n",
    "    tokens: torch.Tensor, shuffle_idxs: torch.Tensor = None\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Shuffle tokens before assigning them to experts\"\"\"\n",
    "    if shuffle_idxs is None:\n",
    "        shuffle_idxs = torch.randperm(tokens.shape[0], device=tokens.device)\n",
    "    return tokens[shuffle_idxs], shuffle_idxs\n",
    "\n",
    "\n",
    "tokens = torch.arange(10) * 10\n",
    "print(\"Original tokens:\")\n",
    "print(tokens)\n",
    "print(\"Shuffled tokens:\")\n",
    "print(shuffle_tokens(tokens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Full switch router"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final output of our router function should contain `expert_assignments` of shape `(num_experts, expert_capacity)`, corresponding token-expert scores `expert_gates` of the same shape, and the `balancing_loss` which we will use during training to ensure balanced token assignments across experts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define two different versions of the switch router, depending if the model is run in `training` or in `inference` mode:\n",
    "* `training`: Expert capacity is calculated as `(batch_size / num_experts) * capacity_factor` and we keep track of `balancing_loss` in order to train the router.\n",
    "* `inference`: We set the expert capacity to the maximum number of tokens assigned to an expert, thus ensuring that no tokens are dropped, while disregarding `balancing_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RouterOutput:\n",
    "    expert_assignments: torch.Tensor\n",
    "    expert_gates: torch.Tensor\n",
    "    balancing_loss: Optional[torch.Tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expert_gates(\n",
    "    scores: torch.Tensor, expert_assignments: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    pad_token = torch.zeros(\n",
    "        (1, scores.shape[1]), dtype=scores.dtype, device=scores.device\n",
    "    )\n",
    "    scores_padded = torch.cat((scores, pad_token), 0)\n",
    "    return torch.gather(scores_padded.transpose(0, 1), 1, expert_assignments)\n",
    "\n",
    "\n",
    "# Test get_expert_gates\n",
    "scores = torch.tensor(\n",
    "    [\n",
    "        [0.1, 0.2, 0.5, 0.2],\n",
    "        [0, 0, 0.9, 0.1],\n",
    "        [0.4, 0.3, 0.15, 0.15],\n",
    "        [0.2, 0.2, 0.2, 0.4],\n",
    "    ]\n",
    ")\n",
    "\n",
    "expert_assignments = torch.tensor([[0], [2], [1], [4]])\n",
    "print(\"Expert assignments:\\n\", expert_assignments)\n",
    "print(\"Corresponding expert gates:\\n\", get_expert_gates(scores, expert_assignments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_router(\n",
    "    scores: torch.Tensor, capacity_factor: float = 1.0, training: bool = True\n",
    ") -> RouterOutput:\n",
    "    expert_mask = get_expert_mask(scores)\n",
    "\n",
    "    # At inference increase capacity so no tokens are dropped\n",
    "    if not training:\n",
    "        capacity = expert_mask.sum(dim=0).max()\n",
    "        expert_assignments = mask_to_expert_assignments(expert_mask, capacity)\n",
    "        expert_gates = get_expert_gates(scores, expert_assignments)\n",
    "        return RouterOutput(expert_assignments, expert_gates, None)\n",
    "\n",
    "    balancing_loss = get_balancing_loss(scores, expert_mask)\n",
    "    expert_mask_shuffled, shuffle_idxs = shuffle_tokens(expert_mask)\n",
    "    batch_size, num_experts = scores.shape\n",
    "    capacity = int(batch_size * capacity_factor / num_experts)\n",
    "    expert_assignments_shuffled = mask_to_expert_assignments(\n",
    "        expert_mask_shuffled, capacity\n",
    "    )\n",
    "    shuffle_idxs_with_pad = torch.cat(\n",
    "        (\n",
    "            shuffle_idxs,\n",
    "            torch.tensor(\n",
    "                [scores.shape[0]], dtype=shuffle_idxs.dtype, device=shuffle_idxs.device\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    expert_assignments = shuffle_idxs_with_pad[expert_assignments_shuffled]\n",
    "    expert_gates = get_expert_gates(scores, expert_assignments)\n",
    "    return RouterOutput(expert_assignments, expert_gates, balancing_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test switch router (training)\n",
    "scores = torch.tensor(\n",
    "    [\n",
    "        [0.1, 0.2, 0.5, 0.2],\n",
    "        [0.0, 0.0, 0.9, 0.1],\n",
    "        [0.4, 0.3, 0.15, 0.15],\n",
    "        [0.2, 0.2, 0.2, 0.4],\n",
    "    ]\n",
    ")\n",
    "\n",
    "r = switch_router(scores, capacity_factor=1.0)\n",
    "\n",
    "print(\"Expert assignments:\\n\", r.expert_assignments)\n",
    "print(\"Expert gates:\\n\", r.expert_gates)\n",
    "print(\"Balancing loss:\\n\", r.balancing_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test switch router (inference)\n",
    "scores = torch.tensor(\n",
    "    [\n",
    "        [0.7, 0.1, 0.1, 0.1],\n",
    "        [0.7, 0.1, 0.1, 0.1],\n",
    "        [0.7, 0.1, 0.1, 0.1],\n",
    "        [0.2, 0.2, 0.2, 0.4],\n",
    "    ]\n",
    ")\n",
    "\n",
    "r = switch_router(scores, training=False)\n",
    "\n",
    "print(\"Expert assignments:\\n\", r.expert_assignments)\n",
    "print(\"Expert gates:\\n\", r.expert_gates)\n",
    "print(\"Balancing loss:\\n\", r.balancing_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token routing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having calculated the token-expert assignments using the `switch_router` function, we now need to route the tokens to their allocated experts. We consider the general case where the experts are distributed equally across our replicas, so that each replica holds `num_experts_per_replica` experts, and the total number of experts is therefore `num_experts = num_replicas * num_experts_per_replica`.\n",
    "\n",
    "When using a single replica (either CPU or a single IPU), all experts live on the device and token routing simply gathers the appropriate tokens for each expert. When using multiple replicas however, each replica will hold a different subset of experts and we need to do cross-device communication in order to send the tokens to the correct device."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All-to-all"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `num_replicas > 1`, each replica will send a subset of its token batch to each other replica. For this we will use the `all_to_all` op from the `poptorch_experimental_addons` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poptorch_experimental_addons.collectives import all_to_all_single_cross_replica\n",
    "\n",
    "\n",
    "def all_to_all(x: torch.Tensor) -> torch.Tensor:\n",
    "    if poptorch.isRunningOnIpu():\n",
    "        return all_to_all_single_cross_replica(x, x.shape[0])\n",
    "    else:\n",
    "        assert x.shape[0] == 1, \"num_replicas needs to be 1 when running on CPU\"\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example showcases `all_to_all` op: we start from the input `x` that is distributed across the `num_replicas` replicas, such that `x[i]` is the tensor on replica `i`. On each replica, the tensor `x[i]` needs to have the shape `(num_replicas, *)` before applying `all_to_all`, so that `x[i][j]` represents the data that replica `i` will send to replica `j`. After the all-to-all operation each replica `i` will have the tensor `y[i]`, such that `y[i][j] = x[j][i]`.\n",
    "\n",
    "Note that by default when we send an input `x` with shape `(batch_size, *)` to a `poptorch_model` it will be distributed across the replicas so that each replica receives an independent slice of the input that will have the shape `(batch_size // num_replicas, *)`, which is why we do an additional reshape to have the input in this form. Similarly, the output will then have the shape `(batch_size, *)` which we reshape to `(num_replicas, batch_size // num_replicas, *)` in order to separate the outputs coming from each replica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all_to_all():\n",
    "    num_replicas = 4\n",
    "\n",
    "    x = np.array(\n",
    "        [\n",
    "            np.arange(1, num_replicas + 1, dtype=np.int32)[:, np.newaxis] * 10**i\n",
    "            for i in range(num_replicas)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for i, x_i in enumerate(x):\n",
    "        print(f\"Input on replica {i}:\")\n",
    "        print(x_i)\n",
    "\n",
    "    x = x.reshape(x.shape[0] * x.shape[1], *x.shape[2:])\n",
    "\n",
    "    class AllToAll(nn.Module):\n",
    "        def forward(self, x: torch.Tensor):\n",
    "            return all_to_all(x)\n",
    "\n",
    "    model = AllToAll()\n",
    "    opts = poptorch.Options()\n",
    "    opts.replicationFactor(num_replicas)\n",
    "    poptorch_model = poptorch.inferenceModel(model, opts)\n",
    "    y = poptorch_model(torch.from_numpy(x))\n",
    "\n",
    "    y = y.reshape((num_replicas, num_replicas, -1)).numpy()\n",
    "\n",
    "    for i, y_i in enumerate(y):\n",
    "        print(f\"Output on replica {i}:\")\n",
    "        print(y_i)\n",
    "\n",
    "\n",
    "# Uncomment to test all-to-all on 4 IPUs\n",
    "# test_all_to_all()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Routing tokens to experts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the `all_to_all` communication, we can route the tokens to the appropriate experts based on the previously calculated `expert_assignments`. We need to ensure that the tokens have the correct shape before the `all_to_all` (`num_replicas, num_experts_per_replica, expert_capacity, hidden_size`), as well as add the padding token for the under-assigned experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_to_replica(\n",
    "    x: torch.Tensor, expert_assignments: torch.Tensor, num_replicas: int\n",
    ") -> torch.Tensor:\n",
    "    assert len(x.shape) == 2\n",
    "\n",
    "    pad_token = torch.zeros((1, x.shape[1]), dtype=x.dtype, device=x.device)\n",
    "    x_padded = torch.cat((x, pad_token), 0)\n",
    "    x_gathered = x_padded[\n",
    "        expert_assignments.reshape(\n",
    "            num_replicas,\n",
    "            expert_assignments.shape[0] // num_replicas,\n",
    "            *expert_assignments.shape[1:]\n",
    "        ),\n",
    "        :,\n",
    "    ]\n",
    "    return all_to_all(x_gathered)\n",
    "\n",
    "\n",
    "# Test route\n",
    "tokens = (torch.arange(8) * 2).reshape(-1, 1)\n",
    "print(\"Tokens:\\n\", tokens)\n",
    "e_assignments = torch.tensor([[1, 3], [0, 7], [5, 6], [2, 4]])\n",
    "print(\"Expert assignments:\\n\", e_assignments)\n",
    "out = route_to_replica(tokens, e_assignments, num_replicas=1)\n",
    "out_expected = torch.tensor([[[2], [6]], [[0], [14]], [[10], [12]], [[4], [8]]])[\n",
    "    None, :, :, :\n",
    "]\n",
    "\n",
    "print(\"Routing output:\\n\", out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rearrange expert inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the `all_to_all`, each replica has a different set of inputs of the shape `x_routed = (num_replicas, num_experts_per_replica, expert_capacity, hidden_size)`. To support multiple experts per replica, within each replica we will run the experts concurrently using block-diagonal matrix multiplication for which we need to re-arrange the inputs to `x_allocated = (num_experts_per_replica, expert_capacity * num_replicas, hidden_size)`. We need to be careful here when reshaping in order to ensure that `x_allocated[i]` collects all tokens sent to expert `i` of the replica, i.e. `x_allocated[i] = x[:][i]`. For this, we need two functions: one to re-arrange expert inputs after `all_to_all`, and one to re-arrange the inputs back to the original form to be sent back after expert processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_to_block(x: torch.Tensor) -> torch.Tensor:\n",
    "    assert len(x.shape) == 4\n",
    "    return x.swapaxes(0, 1).reshape(x.shape[1], -1, x.shape[-1])\n",
    "\n",
    "\n",
    "def unroute_from_block(x: torch.Tensor, num_replicas: int) -> torch.Tensor:\n",
    "    assert len(x.shape) == 3\n",
    "    return x.reshape(x.shape[0], num_replicas, -1, x.shape[-1]).swapaxes(0, 1)\n",
    "\n",
    "\n",
    "# Test allocate-deallocate replica experts\n",
    "x = torch.tensor([[[[0]], [[1]]], [[[2]], [[3]]]])\n",
    "print(\"Input before rearranging:\\n\", x)\n",
    "print(\n",
    "    \"Input shape is (num_replicas, num_experts_per_replica, expert_capacity, hidden_size):\\n\",\n",
    "    x.shape,\n",
    ")\n",
    "y = route_to_block(x)\n",
    "print(\n",
    "    \"Rearranging to (num_experts_per_replica, num_replicas * expert_capacity, hidden_size):\"\n",
    ")\n",
    "y_expected = np.array([[[0], [2]], [[1], [3]]])\n",
    "print(y)\n",
    "print(\"Shape of the output:\\n\", y.shape)\n",
    "_x = unroute_from_block(y, num_replicas=2)\n",
    "print(\"Inverting the re-arrangement:\\n\", _x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Routing back"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the tokens have passed through the expert layers, they need to be rearranged and routed back to their original devices (i.e. the inverse operation of `route`). One addition here is that we need to multiply the expert outputs by the appropriate token-expert score so that we introduce the training signal for the router."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroute_from_replica(\n",
    "    x: torch.Tensor,\n",
    "    expert_assignments: torch.Tensor,\n",
    "    batch_size: int,\n",
    "    expert_gates: Optional[torch.Tensor] = None,\n",
    ") -> torch.Tensor:\n",
    "    assert len(x.shape) == 4\n",
    "\n",
    "    hidden_size = x.shape[-1]\n",
    "    x_unrouted = all_to_all(\n",
    "        x\n",
    "    )  # (num_replicas, num_experts_per_replica, expert_capacity, hidden_size)\n",
    "    if expert_gates is not None:\n",
    "        x_unrouted *= expert_gates.reshape(*x_unrouted.shape[:-1], 1)\n",
    "    x_flat = x_unrouted.reshape(-1, hidden_size)\n",
    "    expert_assignments_flat = expert_assignments.reshape(-1, 1)\n",
    "    y = torch.zeros((batch_size + 1, hidden_size), dtype=x.dtype, device=x.device)\n",
    "    return torch.scatter_reduce(\n",
    "        y, 0, expert_assignments_flat.expand(-1, hidden_size), x_flat, \"sum\"\n",
    "    )[:-1]\n",
    "\n",
    "\n",
    "# Test unroute without expert gates\n",
    "x = torch.tensor(\n",
    "    [[[2], [6]], [[0], [14]], [[10], [12]], [[4], [8]]], dtype=torch.float32\n",
    ")[None, :, :, :]\n",
    "print(\"Tokens to be routed back:\\n\", x)\n",
    "e_assignments = torch.tensor([[1, 3], [0, 3], [5, 6], [2, 1]])\n",
    "print(\"Based on expert assignments:\\n\", e_assignments)\n",
    "y = unroute_from_replica(x, e_assignments, batch_size=np.prod(x.shape[:-1]))\n",
    "y_expected = torch.tensor([0, 2 + 8, 4, 6 + 14, 0, 10, 12, 0]).reshape(-1, 1)\n",
    "print(\"Tokens after unroute (no gating):\\n\", y)\n",
    "\n",
    "# Test unroute with expert gates\n",
    "x = torch.tensor(\n",
    "    [[[2], [6]], [[0], [14]], [[10], [12]], [[4], [8]]], dtype=torch.float32\n",
    ")[None, :, :, :]\n",
    "e_assignments = torch.tensor([[1, 3], [0, 3], [5, 6], [2, 1]])\n",
    "expert_gates = torch.tensor([[1, 1], [1, 1], [0.3, 1], [1, 0.5]])\n",
    "print(\"\\nSame expert assignments but with corresponding expert gates:\\n\", expert_gates)\n",
    "y = unroute_from_replica(\n",
    "    x, e_assignments, batch_size=np.prod(x.shape[:-1]), expert_gates=expert_gates\n",
    ")\n",
    "y_expected = torch.tensor([0, 2 + 0.5 * 8, 4, 6 + 14, 0, 0.3 * 10, 12, 0]).reshape(\n",
    "    -1, 1\n",
    ")\n",
    "print(\"Tokens after unrouted (with gating):\\n\", y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a MoE layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the routing functions we defined to create expert-parallel layers. Let's create `ExpertFFN` - an expert-parallel version of our previous `FFN` layer. For each weight of the original layer of shape `w_shape` we now initialise an expert-parallel version instead, with shape `(num_replicas, num_experts_per_replica, *w_shape)` - in case of the `FFN` layer these are the weights `w1` and `w2`.\n",
    "\n",
    "Note that each replica will only receive its own shard of this weight, so within the forward method of our class the shape will actually be `num_experts_per_replica, *w_shape` without the leading dimension. We need to tell `poptorch` during model definition which weights will be sharded in this way, as by default same weight values are *replicated* for each replica. We get rid of the leading `num_replicas` dimension in the case when `num_replicas == 1`.\n",
    "\n",
    "We can now conduct our matrix multiplications as *batched* matmuls, as our inputs will have the same leading dimension `num_experts_per_replica` after the routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpertFFN(nn.Module):\n",
    "    \"\"\"Transformer-style FFN layer with expert-parallelism\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        expand_factor: int,\n",
    "        dtype: torch.dtype,\n",
    "        num_replicas: int,\n",
    "        num_experts_per_replica: int,\n",
    "        capacity_factor: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.expand_factor = expand_factor\n",
    "        self.dtype = dtype\n",
    "        self.num_replicas = num_replicas\n",
    "        self.num_experts_per_replica = num_experts_per_replica\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.scorer = Scorer(hidden_size, num_replicas * num_experts_per_replica, dtype)\n",
    "        self.balancing_loss = None\n",
    "\n",
    "        # Create expert params\n",
    "        if num_replicas > 1:\n",
    "            e_shape = (num_replicas, num_experts_per_replica)\n",
    "        else:\n",
    "            e_shape = (num_experts_per_replica,)\n",
    "\n",
    "        self.w1 = create_weight_tensor(\n",
    "            (\n",
    "                *e_shape,\n",
    "                hidden_size,\n",
    "                hidden_size * expand_factor,\n",
    "            ),\n",
    "            dtype,\n",
    "        )\n",
    "        self.w2 = create_weight_tensor(\n",
    "            (\n",
    "                *e_shape,\n",
    "                hidden_size * expand_factor,\n",
    "                hidden_size,\n",
    "            ),\n",
    "            dtype,\n",
    "        )\n",
    "\n",
    "        # Keep track of router decisions\n",
    "        self.expert_ids = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Make sure inputs are of shape (batch_size, hidden_size)\n",
    "        x_shape = x.shape\n",
    "        x = x.reshape(-1, x_shape[-1])\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # Assign tokens to experts\n",
    "        probs = self.scorer(x)\n",
    "        self.expert_ids = probs.argmax(dim=1)\n",
    "        r = switch_router(probs, self.capacity_factor, self.training)\n",
    "        self.balancing_loss = r.balancing_loss\n",
    "\n",
    "        # Route + process + unroute\n",
    "        x_routed = route_to_replica(x, r.expert_assignments, self.num_replicas)\n",
    "        x_allocated = route_to_block(x_routed)\n",
    "        y = F.relu(x_allocated @ self.w1) @ self.w2  # batched matmul\n",
    "        y_deallocated = unroute_from_block(y, self.num_replicas)\n",
    "        return unroute_from_replica(\n",
    "            y_deallocated, r.expert_assignments, batch_size, r.expert_gates\n",
    "        ).reshape(x_shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a function that will take a regular dense `FFN` and return an expert-parallel `ExpertFFN` that we can later use to turn dense layers into MoE layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moe_ffn_like(\n",
    "    ffn: FFN, num_replicas: int, num_experts_per_replica: int, capacity_factor: float\n",
    "):\n",
    "    return ExpertFFN(\n",
    "        hidden_size=ffn.hidden_size,\n",
    "        expand_factor=ffn.expand_factor,\n",
    "        dtype=ffn.dtype,\n",
    "        num_replicas=num_replicas,\n",
    "        num_experts_per_replica=num_experts_per_replica,\n",
    "        capacity_factor=capacity_factor,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test expert ffn\n",
    "ffn = FFN(hidden_size=1, expand_factor=4, dtype=torch.float32)\n",
    "e_ffn = moe_ffn_like(\n",
    "    ffn, num_replicas=1, num_experts_per_replica=5, capacity_factor=1.0\n",
    ")\n",
    "\n",
    "\n",
    "# Test shapes\n",
    "assert e_ffn.w1.shape == (5, 1, 1 * 4)\n",
    "assert e_ffn.w2.shape == (5, 1 * 4, 1)\n",
    "assert e_ffn.scorer.w.shape == (1, 5)\n",
    "assert e_ffn.scorer.b.shape == (5,)\n",
    "x = torch.randn(15, 1)\n",
    "y = e_ffn(x)\n",
    "assert y.shape == x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MoE character model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now update our original character language model by replacing some of the `FFN` layers with our new `ExpertFFN` layers! A common approach to \"sparsifying\" a dense model is to take every other layer and make it MoE - so let's do that.\n",
    "\n",
    "In addition to exchanging some `FFN` layers with `ExpertFFN` layers, we need to ensure that the expert balancing loss is included in the final loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoeCharModel(CharModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        hidden_size: int,\n",
    "        expand_factor: int,\n",
    "        kernel_size: int,\n",
    "        num_groups: int,\n",
    "        num_layers: int,\n",
    "        dtype: torch.dtype,\n",
    "        num_replicas: int,\n",
    "        num_experts_per_replica: int,\n",
    "        capacity_factor: float,\n",
    "        moe_layers: Tuple[int, ...],\n",
    "        aux_loss_weight: float,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=hidden_size,\n",
    "            expand_factor=expand_factor,\n",
    "            kernel_size=kernel_size,\n",
    "            num_groups=num_groups,\n",
    "            num_layers=num_layers,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "        self.moe_layers = moe_layers\n",
    "        self.aux_loss_weight = aux_loss_weight\n",
    "\n",
    "        # Convert layers to MoE\n",
    "        for i in moe_layers:\n",
    "            self.core_layers[i].ffn = moe_ffn_like(\n",
    "                self.core_layers[i].ffn,\n",
    "                num_replicas,\n",
    "                num_experts_per_replica,\n",
    "                capacity_factor,\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, mask: torch.Tensor = None, y: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        out = super().forward(x, mask, y)\n",
    "        if self.training:\n",
    "            logits, cross_entropy_loss = out\n",
    "            aux_losses = {\n",
    "                i: self.core_layers[i].ffn.balancing_loss\n",
    "                for i in self.moe_layers\n",
    "                if self.core_layers[i].ffn.balancing_loss is not None\n",
    "            }\n",
    "            aux_loss = sum(aux_losses.values())\n",
    "            loss = cross_entropy_loss + self.aux_loss_weight * aux_loss\n",
    "            return {\n",
    "                \"logits\": logits,\n",
    "                \"cross entropy loss\": cross_entropy_loss,\n",
    "                \"aux losses\": aux_losses,\n",
    "            }, poptorch.identity_loss(loss, reduction=\"none\")\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training MoE model (*can be skipped*)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train our new model! We need to ensure that our expert parameters are properly sharded across the replicas - we will use this by setting the appropriate communication groups for the parameters of `ExpertFFN` layers, but making sure that the `Scorer` parameters are kept replicated across the replicas (as the router should be shared across each device)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
        "jupyter": {
            "source_hidden": true
        }
   },
   "outputs": [],
   "source": [
    "# # Load the training dataset\n",
    "\n",
    "# vocab_path = dataset_dir / \"wikitext103_raw/vocab.json\"\n",
    "# train_path = dataset_dir / \"wikitext103_raw/train.txt\"\n",
    "\n",
    "# train_ds = Dataset.from_path(vocab_path, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
        "jupyter": {
            "source_hidden": true
        }
   },
   "outputs": [],
   "source": [
    "# # Set random seed\n",
    "# seed = None\n",
    "# if seed is None:\n",
    "#     seed = np.random.SeedSequence().generate_state(1)[0]\n",
    "\n",
    "# # Model hyperparameters\n",
    "# num_replicas = 4\n",
    "# num_experts_per_replica = 4\n",
    "# model = MoeCharModel(\n",
    "#     vocab_size=len(ds.vocab),\n",
    "#     hidden_size=128,\n",
    "#     expand_factor=4,\n",
    "#     kernel_size=7,\n",
    "#     num_groups=8,\n",
    "#     num_layers=8,\n",
    "#     dtype=torch.float32,\n",
    "#     num_replicas=num_replicas,\n",
    "#     num_experts_per_replica=num_experts_per_replica,\n",
    "#     capacity_factor=1.0,\n",
    "#     moe_layers=(0, 2, 4, 6),\n",
    "#     aux_loss_weight=0.01,\n",
    "# )\n",
    "\n",
    "# # Batch settings\n",
    "# batch_size = 32\n",
    "# sequence_length = 256\n",
    "# overlap_length = 32\n",
    "# batch_seed = 100\n",
    "\n",
    "\n",
    "# # Change this to switch between IPU and CPU execution\n",
    "# run_on_ipu = True\n",
    "\n",
    "# # Training parameters\n",
    "# lr = 2e-3\n",
    "# lr_decay = 2**-16\n",
    "# num_steps = 2**15\n",
    "\n",
    "\n",
    "# # Set the optimizer\n",
    "# if run_on_ipu:\n",
    "#     opts = poptorch.Options()\n",
    "#     opts.replicationFactor(num_replicas)\n",
    "#     optimizer = poptorch.optim.AdamW(model.parameters(), lr=lr)\n",
    "#     poptorch_model = poptorch.trainingModel(model, opts, optimizer)\n",
    "\n",
    "#     # Shard MoE params across replicas\n",
    "#     if num_replicas > 1:\n",
    "#         for i in model.moe_layers:\n",
    "#             for p_name, _ in model.core_layers[i].ffn.named_parameters():\n",
    "#                 # Router params should stay replicated\n",
    "#                 if \"scorer\" not in p_name:\n",
    "#                     poptorch_model.per_replica_params[f\"core_layers.{i}.ffn.\" + p_name] = (\n",
    "#                         poptorch.enums.CommGroupType.NoGrouping,\n",
    "#                         0,\n",
    "#                         poptorch.enums.VariableRetrievalMode.OnePerGroup,\n",
    "#                     )\n",
    "\n",
    "\n",
    "# else:\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# def mean_across_replicas(out: dict, num_replicas: int) -> dict:\n",
    "#     \"\"\"Average output values across replicas\"\"\"\n",
    "#     if num_replicas == 1:\n",
    "#         return out\n",
    "\n",
    "#     return {\n",
    "#         key: (\n",
    "#             torch.mean(\n",
    "#                 val.reshape(\n",
    "#                     (num_replicas, val.shape[0] // num_replicas, *val.shape[1:])\n",
    "#                 ),\n",
    "#                 dim=0,\n",
    "#                 dtype=torch.float32,\n",
    "#             )\n",
    "#             if type(val) is not dict\n",
    "#             else mean_across_replicas(val, num_replicas)\n",
    "#         )\n",
    "#         for key, val in out.items()\n",
    "#     }\n",
    "\n",
    "\n",
    "# # Training loop\n",
    "# model.train()\n",
    "# lr_decay_factor = 2**-lr_decay\n",
    "# for step, batch in zip(\n",
    "#     range(num_steps),\n",
    "#     train_ds.batch(batch_size, sequence_length, overlap_length, batch_seed),\n",
    "# ):\n",
    "#     # Apply a step\n",
    "#     if run_on_ipu:\n",
    "#         out, _ = poptorch_model(batch[\"x\"].int(), batch[\"mask\"].int(), batch[\"y\"].int())\n",
    "#         out = mean_across_replicas(out, num_replicas)\n",
    "#     else:\n",
    "#         optimizer.zero_grad()\n",
    "#         out, loss = model(batch[\"x\"], batch[\"mask\"], batch[\"y\"])\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     # Print results\n",
    "#     if step % 2**8 == 0:\n",
    "#         print(f\"Step {step}:\")\n",
    "#         print(f\"Loss: {out['cross entropy loss']}\")\n",
    "#         print(f\"Expert balancing losses: {out['aux losses']}\")\n",
    "\n",
    "#     # Learning rate schedule\n",
    "#     lr *= lr_decay_factor\n",
    "#     optimizer.param_groups[0][\"lr\"] = lr\n",
    "#     if run_on_ipu:\n",
    "#         poptorch_model.setOptimizer(optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test how our models perform and observe the effect of increasing the number of experts on the model performance. For this, we can either use models trained in the notebook, or one of the pre-trained models provided. We include pre-trained MoE models with 4, 8, and 16 experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: CharModel, ds: Dataset) -> float:\n",
    "    model.train(True)\n",
    "\n",
    "    # Batch settings\n",
    "    batch_size = 32\n",
    "    sequence_length = 256\n",
    "    overlap_length = 32\n",
    "    batch_seed = None\n",
    "\n",
    "    opts = poptorch.Options()\n",
    "    num_replicas = 4\n",
    "    opts.replicationFactor(num_replicas)\n",
    "    optimizer = poptorch.optim.SGD(model.parameters(), lr=0)\n",
    "    poptorch_model = poptorch.trainingModel(model, opts, optimizer)\n",
    "    if num_replicas > 1 and isinstance(model, MoeCharModel):\n",
    "        for i in model.moe_layers:\n",
    "            for p_name, _ in model.core_layers[i].ffn.named_parameters():\n",
    "                if \"scorer\" not in p_name:\n",
    "                    poptorch_model.per_replica_params[\n",
    "                        f\"core_layers.{i}.ffn.\" + p_name\n",
    "                    ] = (\n",
    "                        poptorch.enums.CommGroupType.NoGrouping,\n",
    "                        0,\n",
    "                        poptorch.enums.VariableRetrievalMode.OnePerGroup,\n",
    "                    )\n",
    "\n",
    "    losses = []\n",
    "    for batch in ds.batch(batch_size, sequence_length, overlap_length, batch_seed):\n",
    "        out, loss = poptorch_model(\n",
    "            batch[\"x\"].int(), batch[\"mask\"].int(), batch[\"y\"].int()\n",
    "        )\n",
    "        if isinstance(model, MoeCharModel):\n",
    "            loss = out[\"cross entropy loss\"]\n",
    "        losses.append(torch.mean(loss))\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "vocab_path = dataset_dir / \"wikitext103_raw/vocab.json\"\n",
    "test_path = dataset_dir / \"wikitext103_raw/test.txt\"\n",
    "test_ds = Dataset.from_path(vocab_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate dense model\n",
    "\n",
    "# model = CharModel(\n",
    "#     vocab_size=len(ds.vocab),\n",
    "#     hidden_size=128,\n",
    "#     expand_factor=4,\n",
    "#     kernel_size=7,\n",
    "#     num_groups=8,\n",
    "#     num_layers=8,\n",
    "#     dtype=torch.float32,\n",
    "# )\n",
    "# model.load_state_dict(torch.load(checkpoint_dir / \"dense.pt\"))\n",
    "\n",
    "# print(f\"Baseline model loss on test set: {evaluate_model(model, test_ds).item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate MoE models\n",
    "\n",
    "# # Set number of experts (4, 8, or 16)\n",
    "# num_experts = 4\n",
    "\n",
    "# model = MoeCharModel(\n",
    "#     vocab_size=len(ds.vocab),\n",
    "#     hidden_size=128,\n",
    "#     expand_factor=4,\n",
    "#     kernel_size=7,\n",
    "#     num_groups=8,\n",
    "#     num_layers=8,\n",
    "#     dtype=torch.float32,\n",
    "#     num_replicas=4,\n",
    "#     num_experts_per_replica=num_experts // 4,\n",
    "#     capacity_factor=1.0,\n",
    "#     moe_layers=(0, 2, 4, 6),\n",
    "#     aux_loss_weight=0.01,\n",
    "# )\n",
    "\n",
    "# model.load_state_dict(torch.load(checkpoint_dir / f\"moe-{num_experts}.pt\"))\n",
    "\n",
    "# print(\n",
    "#     f\"MoE model ({num_experts} experts) loss on test set: {evaluate_model(model, test_ds).item():.3f}\"\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should observe that increasing the number of experts indeed improved the performance of the model! Note that importantly, this improvement comes without a proportional increase in compute cost - mixture of expert scheme only adds the routing logic overhead, but the processing FLOPs per token remain fixed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running MoE inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see our MoE models in action! We will run the inference on the CPU, so we need to make sure that any expert weights of the shape `(num_replicas, num_experts_per_replica, ...)` are reshaped to `(num_replicas * num_experts_per_replica, ...)` as expected by the CPU model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_expert_weights(state_dict: dict, moe_layers: Tuple[int, ...]) -> dict:\n",
    "    for i in moe_layers:\n",
    "        w1 = state_dict[f\"core_layers.{i}.ffn.w1\"]\n",
    "        w2 = state_dict[f\"core_layers.{i}.ffn.w2\"]\n",
    "        state_dict[f\"core_layers.{i}.ffn.w1\"] = w1.reshape(-1, *w1.shape[2:])\n",
    "        state_dict[f\"core_layers.{i}.ffn.w2\"] = w2.reshape(-1, *w2.shape[2:])\n",
    "    return state_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also keep track of the router decisions by colouring each input character based on the expert it was routed to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_text(text: str, id: int) -> str:\n",
    "    col_id = id + 1 if id < 15 else id + 20\n",
    "    return \"\\033[38;5;232m\" + f\"\\033[48;5;{col_id}m\" + text + \"\\033[0;0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of experts (can be 4, 8 or 16)\n",
    "num_experts = 4\n",
    "\n",
    "model_from_checkpoint = MoeCharModel(\n",
    "    vocab_size=len(ds.vocab),\n",
    "    hidden_size=128,\n",
    "    expand_factor=4,\n",
    "    kernel_size=7,\n",
    "    num_groups=8,\n",
    "    num_layers=8,\n",
    "    dtype=torch.float32,\n",
    "    num_replicas=1,\n",
    "    num_experts_per_replica=num_experts,\n",
    "    capacity_factor=1.0,\n",
    "    moe_layers=(0, 2, 4, 6),\n",
    "    aux_loss_weight=0.01,\n",
    ")\n",
    "state_dict = flatten_expert_weights(\n",
    "    torch.load(checkpoint_dir / f\"moe-{num_experts}.pt\"), moe_layers=(0, 2, 4, 6)\n",
    ")\n",
    "model_from_checkpoint.load_state_dict(state_dict)\n",
    "\n",
    "in_text = \"This is a sentence to see how characters are routed to experts.\\nSome numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9.\\n \"\n",
    "output = sample_text(\n",
    "    model_from_checkpoint,\n",
    "    in_text,\n",
    "    num_characters=1,\n",
    "    temperature=0.3,\n",
    ")\n",
    "for i in (0, 2, 4, 6):\n",
    "    print(f\"Layer {i}:\")\n",
    "    expert_ids = model_from_checkpoint.core_layers[i].ffn.expert_ids.numpy()\n",
    "    colored_text = \"\"\n",
    "    for c, id in zip(output, expert_ids):\n",
    "        colored_text += wrap_text(c, id)\n",
    "    print(colored_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provided a tutorial on how to define a model that uses a mixture of experts execution scheme to increase the model capacity without a proportional increase in computational cost. Although the notebook covered defining a character-level language model, the techniques described here can be easily applied to making arbitrary expert-parallel layers. Moreover, by simply exchanging the FFN layers with `ExpertFFN` layers defined here and adding the expert-balancing loss to our final loss calculation, we can adapt arbitrary transformer-like models to utilise mixture of experts and run on both IPUs and CPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.2.0-EA.1+1218_poptorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c23b2585764a5cea999a38a63cb5de975e465d0e20532e67163f65bb712149b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
