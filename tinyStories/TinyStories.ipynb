{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Small Language Models on TinyStories dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TinyStories is a dataset with constrained vocabulary and training information, making it a valuable resource for researchers who are interested in studying how language models work and perform tasks such as writing coherent English text, in-context reasoning, and creative writing. The dataset can be used to train various small language models, which require little resources and time to do full pre-training yet produce fluent and coherent English utterances. \n",
    "\n",
    "TinyStories is made up of short stories written in words that typical 3 to 4-year-olds would usually understand in a style of fairy tales; it was generated by GPT-3.5 and GPT-4. The dataset consists of 2.1 million short stories, each of which is only a few sentences long. The stories are all about everyday events that a young child might experience, such as going to the park, playing with friends, or learning something new.\n",
    "\n",
    "IPU is a great platform to experiment with TinyStories dataset and research novel techniques and architectures, due IPU's acceleration capability for small models. IPU relies of very fast SRAM memory and works at its best when the full model can be fit into a single M-2000 appliance consisting of 4 chips, connected by fast IPU Links. The smallest model takes as little as 5 minutes to become capable of generating coherent English sentences. \n",
    "\n",
    "Sample outputs:\n",
    "\n",
    "- TinyStories Model with 5M params (5 mins training - 4IPUs): **This was a sunny day** of the beach. One day, a little girl named Amy was playing in the park. She saw a big tree with lots of leaves. She wanted to pick it up. She picked it up and put it in her pocket. Amy picked up the tree and started to eat it. She was so happy to have a friend to play with. Amy was so happy to have a new friend to play with.\n",
    "\n",
    "\n",
    "- TinyStories Model with 50M params (30 mins training - 4 IPUs): **This was a sunny day** and the little girl was feeling very happy. The girl went to the park and she saw a big tree. She wanted to climb the tree and see what was up there. She started to climb the tree and it was very tall. The little girl kept climbing and climbing until she was very high up. She looked around and saw lots of fun things. She saw a big tree, a pond and a playground. The little girl was having so much fun that she didn't want to leave. But then she heard a voice calling her. It was her mom. The little girl was so happy that she had climbed the tree. She ran to her mom and hugged her. The mom smiled and hugged her back. She was so proud of her little girl. The little girl and her mom went home.\n",
    "\n",
    "\n",
    "- TinyStories Model with 200M params (4h training - 4 IPUs): **This was a sunny day** and there were many kids playing. A little girl named Lily saw a boy named Tim. She walked up to him and said, \"Hi, I'm Lily. Do you want to play with me?\" Tim smiled and said, \"Yes! Let's play together!\" They played on the swings, the slide, and the seesaw. They had so much fun. After playing, Lily and Tim were tired. They sat down on the grass to relax. They talked and laughed together. The sun was warm and the grass was soft. They became good friends and promised to play together again soon.\n",
    "\n",
    "\n",
    "- Off-the-shelf GPT-2 XL 1.5B params: **This was a sunny day** in downtown New York. The sidewalks were packed with people on sidewalks and in front of stores and businesses. There weren't a lot of cars running from building to building at this time of night. The nightlife was in full swing. \n",
    "\n",
    "\n",
    "Arxiv link to paper: [TinyStories: How Small Can Language Models Be and Still Speak Coherent English?](https://arxiv.org/abs/2305.07759)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dowload libraries\n",
    "! pip install datasets matplotlib einops tokenizers wandb pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import dataclasses\n",
    "from IPython import display\n",
    "from dataclasses import dataclass\n",
    "from itertools import islice\n",
    "from typing import *\n",
    "\n",
    "import torch\n",
    "import poptorch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "\n",
    "import datasets\n",
    "import tokenizers\n",
    "\n",
    "import tqdm\n",
    "import wandb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_IPUS = int(os.getenv(\"NUM_AVAILABLE_IPU\", 4))\n",
    "MODEL_SAVING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gc-monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    hidden_size: int\n",
    "    depth: int\n",
    "    seq_length: int\n",
    "    head_size: int\n",
    "    dtype: Literal['half', 'float', 'float16', 'float32']\n",
    "    pipeline_stages: int\n",
    "    checkpointing: bool\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    lr: float\n",
    "    steps: int\n",
    "    batch_size: int\n",
    "    compute_batch_size: int\n",
    "    replicas: int\n",
    "    device_iterations: int\n",
    "    generation_temperature: float\n",
    "    offloading: bool\n",
    "    wd: float = 0.01\n",
    "\n",
    "@dataclass\n",
    "class Experiment:\n",
    "    name: str\n",
    "    model: ModelConfig\n",
    "    train: TrainingConfig\n",
    "    profiling: bool\n",
    "    wandb: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains all the components that make the transformer model. In particular:\n",
    "\n",
    "- attention block\n",
    "- feed-forward block\n",
    "- residual and layer-norm that transform the above components into modules\n",
    "- a transformer layer consisting of two modules\n",
    "- model consisting of embedding, layer stack and de-embedding projection\n",
    "\n",
    "Note: this implementation does not use dropout and uses [ALiBi](https://arxiv.org/abs/2108.12409) positional encoding scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer attention module with relative positional encoding\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.n_heads = config.hidden_size // config.head_size\n",
    "        self.qkv = nn.Linear(config.hidden_size, 3 * self.n_heads * config.head_size)\n",
    "        self.out = nn.Linear(self.n_heads * config.head_size, config.hidden_size)\n",
    "        self.attention_bias = nn.Parameter(self.gen_attention_bias(), requires_grad=False)\n",
    "        \n",
    "    def gen_attention_bias(self) -> Tensor:\n",
    "        causal_mask = torch.tril(torch.ones((self.config.seq_length, self.config.seq_length), dtype=torch.float16))\n",
    "        causal_mask = causal_mask.view(1, 1, self.config.seq_length, self.config.seq_length)\n",
    "        alibi_mask = self.gen_alibi_mask(causal_mask)\n",
    "        causal_mask = (1.0 - causal_mask) * -10_000\n",
    "        return alibi_mask + causal_mask\n",
    "    \n",
    "    # Based on https://nn.labml.ai/transformers/alibi/index.html\n",
    "    def gen_alibi_mask(self, causal_mask: Tensor) -> Tensor:\n",
    "        distances = causal_mask.to(torch.float32).cumsum(dim=-1)\n",
    "        slopes = self.gen_slopes()\n",
    "        return distances.to(torch.float16) * slopes.view(1, self.n_heads, 1, 1)\n",
    "    \n",
    "    def gen_slopes(self) -> Tensor:\n",
    "        n = 2 ** math.floor(math.log2(self.n_heads))\n",
    "        m_0 = 2.0 ** (-8.0 / n)\n",
    "        m = torch.pow(m_0, torch.arange(1, 1 + n))\n",
    "        if n < self.n_heads:\n",
    "            m_hat_0 = 2.0 ** (-4.0 / n)\n",
    "            m_hat = torch.pow(m_hat_0, torch.arange(1, 1 + 2 * (self.n_heads - n), 2))\n",
    "            m = torch.cat([m, m_hat])\n",
    "        return m\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        s = x.shape[1]\n",
    "        q, k, v = einops.rearrange(\n",
    "            self.qkv(x), \"b s (M n d) -> M b n s d\", M=3, n=self.n_heads\n",
    "        )\n",
    "        a = torch.einsum(\"bnsd, bntd -> bnst\", q, k) * q.shape[-1] ** -0.5\n",
    "        a += self.attention_bias[:, :, :s, :s]\n",
    "        mix = torch.einsum(\"bnst, bntd -> bnsd\", torch.softmax(a, -1), v)\n",
    "        return self.out(einops.rearrange(mix, \"b n s d -> b s (n d)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFN module with a GeLU non-linearity\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.up = nn.Linear(config.hidden_size, 4 * config.hidden_size)\n",
    "        self.down = nn.Linear(self.up.out_features, self.up.in_features)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.down(F.gelu(self.up(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization wrapper with a Pre-Norm configuration\n",
    "class PreNormResidual(nn.Module):\n",
    "    def __init__(self, config: ModelConfig, body: nn.Module):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm([config.hidden_size])\n",
    "        self.body = body\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x + self.body(self.norm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer layer\n",
    "class TransformerLayer(nn.Sequential):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__(\n",
    "            PreNormResidual(config, Attention(config)),\n",
    "            PreNormResidual(config, FFN(config)),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, config: ModelConfig, tokenizer: tokenizers.Tokenizer):\n",
    "        super().__init__()\n",
    "        self.config = c = config\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.pre = nn.Sequential(\n",
    "            nn.Embedding(tokenizer.get_vocab_size(), c.hidden_size),\n",
    "            nn.LayerNorm([c.hidden_size]),\n",
    "        )\n",
    "        self.core = nn.Sequential(*(TransformerLayer(config) for _ in range(c.depth)))\n",
    "        self.post = nn.Sequential(\n",
    "            nn.LayerNorm([c.hidden_size]),\n",
    "            nn.Linear(c.hidden_size, tokenizer.get_vocab_size()),\n",
    "        )\n",
    "        self.pre[0] = poptorch.BeginBlock(self.pre[0], \"Token embedding\", ipu_id=0)\n",
    "        for i in range(c.depth):\n",
    "            if c.checkpointing:\n",
    "                self.recomputation_checkpoint(self.core[i])\n",
    "            self.core[i] = poptorch.BeginBlock(\n",
    "                self.core[i], ipu_id=(i * c.pipeline_stages) // c.depth\n",
    "            )\n",
    "        self.post[1] = poptorch.BeginBlock(self.post[1], ipu_id=c.pipeline_stages - 1)\n",
    "        self.model = nn.Sequential(self.pre, self.core, self.post)\n",
    "        self.to(getattr(torch, c.dtype))\n",
    "        \n",
    "    @staticmethod\n",
    "    def recomputation_checkpoint(module: nn.Module) -> torch.utils.hooks.RemovableHandle:\n",
    "        \"\"\"Annotates the output of a module to be checkpointed instead of\n",
    "        recomputed.\"\"\"\n",
    "        def recompute_outputs(module, inputs, outputs):\n",
    "            if isinstance(outputs, torch.Tensor):\n",
    "                return poptorch.recomputationCheckpoint(outputs)\n",
    "            elif isinstance(outputs, tuple):\n",
    "                return tuple(poptorch.recomputationCheckpoint(y) for y in outputs)\n",
    "\n",
    "        module.register_forward_hook(recompute_outputs)\n",
    "\n",
    "    def forward(self, indices: Tensor, debug=False) -> Tensor:\n",
    "        logits = self.model(indices).float()\n",
    "        return F.cross_entropy(logits[:, :-1, :].flatten(0, -2), indices[:, 1:].flatten())\n",
    "\n",
    "    def generate(self, prompt: str, count: int, temperature: float) -> str:\n",
    "        prompt_ids = self.tokenizer.encode(prompt).ids\n",
    "        completion_ids = []\n",
    "        for _ in range(count):\n",
    "            logits = self.model(torch.tensor(prompt_ids + completion_ids)[None])[0, -1]\n",
    "            sample = torch.argmax(\n",
    "                logits + temperature * torch.log(-torch.log(torch.rand_like(logits)))\n",
    "            )\n",
    "            print(self.tokenizer.decode(prompt_ids + completion_ids), end=\"\\r\")\n",
    "            completion_ids.append(int(sample))\n",
    "        print(\"\\n\")\n",
    "        self.forward(torch.tensor(prompt_ids + completion_ids)[None], debug=True)\n",
    "        return self.tokenizer.decode(completion_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Dataset:\n",
    "    data: Dict[str, Tensor]\n",
    "    tokenizer: tokenizers.Tokenizer\n",
    "\n",
    "    def batches(\n",
    "        self, seq_length: int, batch_size: int, split: str\n",
    "    ) -> Iterable[Tensor]:\n",
    "        tokens = self.data[split]\n",
    "        while True:\n",
    "            offsets = torch.randint(\n",
    "                0, len(tokens) - seq_length, size=(batch_size,)\n",
    "            )\n",
    "            yield torch.stack([tokens[i : i + seq_length].long() for i in offsets])\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, vocab_size: int, path: Path) -> \"Dataset\":\n",
    "        if not (path / \"tokenizer.json\").exists() or not (path / \"data.pt\").exists():\n",
    "            path.mkdir(exist_ok=True, parents=True)\n",
    "            original_data = datasets.load_dataset(\"roneneldan/TinyStories\")\n",
    "            tokenizer = tokenizers.Tokenizer(\n",
    "                tokenizers.models.BPE(end_of_word_suffix=\"</w>\")\n",
    "            )\n",
    "            tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.WhitespaceSplit()\n",
    "            tokenizer.decoder = tokenizers.decoders.BPEDecoder(\n",
    "                suffix=tokenizer.model.end_of_word_suffix\n",
    "            )\n",
    "            tokenizer.train_from_iterator(\n",
    "                original_data[\"train\"][: int(1e5)][\"text\"],\n",
    "                tokenizers.trainers.BpeTrainer(\n",
    "                    vocab_size=vocab_size,\n",
    "                    limit_alphabet=512,\n",
    "                    special_tokens=[\"<pad>\", \"</s>\"],\n",
    "                    end_of_word_suffix=tokenizer.model.end_of_word_suffix,\n",
    "                    show_progress=False,\n",
    "                ),\n",
    "            )\n",
    "            tokenizer.save(str(path / \"tokenizer.json\"))\n",
    "            torch.save(\n",
    "                {\n",
    "                    name: torch.tensor(\n",
    "                        [\n",
    "                            token\n",
    "                            for batch in tqdm.tqdm(\n",
    "                                split.iter(1000),\n",
    "                                total=len(split) // 1000,\n",
    "                                desc=f\"generating {name}\",\n",
    "                            )\n",
    "                            for seq in tokenizer.encode_batch(batch[\"text\"])\n",
    "                            for token in [tokenizer.token_to_id(\"</s>\")] + seq.ids\n",
    "                        ],\n",
    "                        dtype=torch.int16,\n",
    "                    )\n",
    "                    for name, split in original_data.items()\n",
    "                },\n",
    "                path / \"data.pt\",\n",
    "            )\n",
    "        return cls(\n",
    "            data=torch.load(path / \"data.pt\"),\n",
    "            tokenizer=tokenizers.Tokenizer.from_file(str(path / \"tokenizer.json\")),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, experiment: Experiment, dataset: Dataset):\n",
    "        self.train_config = tc = experiment.train\n",
    "        self.model_config = mc = experiment.model\n",
    "        self.wandb = experiment.wandb\n",
    "        self.name = experiment.name\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        if experiment.profiling:\n",
    "            self.set_up_profiling(mc, tc)\n",
    "        \n",
    "        torch.seed()\n",
    "        self.options = self.get_poptorch_options(mc, tc)\n",
    "        \n",
    "        \n",
    "        self.model = Model(mc, dataset.tokenizer)\n",
    "        self.optimizer = poptorch.optim.AdamW(\n",
    "            self.model.parameters(), lr=tc.lr, weight_decay=tc.wd, loss_scaling=mc.seq_length, \n",
    "        )\n",
    "        self.host_steps = tc.steps // tc.device_iterations\n",
    "        self.lr_schedule = torch.optim.lr_scheduler.LinearLR(self.optimizer, 1, 0, total_iters=self.host_steps)\n",
    "        self.poptorch_trainer = poptorch.trainingModel(self.model, self.options, self.optimizer)\n",
    "        \n",
    "        if self.wandb:\n",
    "            self.set_up_wandb(experiment)\n",
    "        \n",
    "    @staticmethod\n",
    "    def set_up_profiling(model_config: ModelConfig, train_config: TrainingConfig):\n",
    "        \"\"\"\n",
    "        Enable memory profiling on IPU.\n",
    "        \"\"\"\n",
    "        out = Path(\"profiles/latest\")\n",
    "        out.mkdir(exist_ok=True, parents=True)\n",
    "        os.environ[\"POPLAR_ENGINE_OPTIONS\"] = json.dumps(\n",
    "            {\n",
    "                \"autoReport.outputGraphProfile\": True,\n",
    "                \"autoReport.directory\": str(out),\n",
    "                \"autoReport.outputArchive\": True,\n",
    "                \"debug.outputAllSymbols\": True,\n",
    "            }\n",
    "        )\n",
    "        (out / \"app.json\").write_text(\n",
    "            json.dumps(dict(model=model_config.__dict__, training=train_config.__dict__))\n",
    "        )     \n",
    "\n",
    "    @staticmethod\n",
    "    def get_poptorch_options(model_config: ModelConfig, train_config: TrainingConfig) -> poptorch.Options:\n",
    "        \"\"\"\n",
    "        Configure poptorch options to run the model on IPU\n",
    "        \"\"\"\n",
    "        options = poptorch.Options()\n",
    "        options.output_mode = poptorch.OutputMode.Sum\n",
    "        options.device_iterations = train_config.device_iterations\n",
    "        options.replication_factor = train_config.replicas\n",
    "        options.Training.gradient_accumulation = (\n",
    "            train_config.batch_size // train_config.replicas // train_config.compute_batch_size\n",
    "        )\n",
    "        options.Precision.setPartialsType(torch.half)\n",
    "        options.Precision.enableFloatingPointExceptions(True)\n",
    "        \n",
    "        if train_config.offloading:\n",
    "            proportions = {f\"IPU{ipu}\": 0.4 for ipu in range(model_config.pipeline_stages)}\n",
    "            options.setAvailableMemoryProportion(proportions)\n",
    "            options.TensorLocations.setOptimizerLocation(\n",
    "                poptorch.TensorLocationSettings().useOnChipStorage(False))\n",
    "        \n",
    "        return options\n",
    "    \n",
    "    def set_up_wandb(self, experiment: Experiment)-> None:\n",
    "        os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "        os.environ[\"WANDB_DIR\"] = \"/tmp/wandb\"\n",
    "        Path(\"/tmp/wandb\").mkdir(exist_ok=True)\n",
    "        wandb.init(\n",
    "            project=\"tinystories\",\n",
    "            config=dict(**dataclasses.asdict(experiment)),\n",
    "            reinit=True,\n",
    "        )\n",
    "        wandb.summary[\"n_parameters\"] = sum(p.nelement() for p in self.model.parameters())\n",
    "    \n",
    "    def summary(self) -> None:\n",
    "        print(\n",
    "            f\"Running: {self.name}\"\n",
    "            f\"\\n{self.train_config}\\n{self.model_config}\"\n",
    "            f\"\\n({sum(p.nelement() for p in self.model.parameters())/1e6:.1f} million parameters)\\n\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        \n",
    "    def outputs(self) -> Iterable[Tensor]:\n",
    "        batches = self.dataset.batches(\n",
    "            self.model_config.seq_length,\n",
    "            self.train_config.device_iterations * self.train_config.batch_size,\n",
    "            \"train\",\n",
    "        )\n",
    "        for host_step, batch in enumerate(islice(batches, self.host_steps)):\n",
    "            self.poptorch_trainer.setOptimizer(self.optimizer)\n",
    "            loss = (\n",
    "                self.poptorch_trainer(batch).sum()\n",
    "                * self.train_config.compute_batch_size\n",
    "                / (self.train_config.batch_size * self.train_config.device_iterations)\n",
    "            )\n",
    "            self.lr_schedule.step()\n",
    "            yield host_step * self.train_config.device_iterations, float(loss)\n",
    "\n",
    "            \n",
    "    def _train_generator(self):\n",
    "        it = iter(self.outputs())\n",
    "        yield next(it)  # compile (before tqdm)\n",
    "        yield from tqdm.tqdm(\n",
    "            it, initial=1, total=self.host_steps, desc=\"training\", ncols=120\n",
    "        )\n",
    "            \n",
    "    def train(self, online_plot: bool):\n",
    "        training_loss = []\n",
    "        colour = col = np.random.rand(3)\n",
    "        \n",
    "        if online_plot:\n",
    "            fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "        for step, loss in self._train_generator():\n",
    "            training_loss.append(loss)\n",
    "            \n",
    "            if self.wandb:\n",
    "                wandb.log(dict(loss=loss), step=step)\n",
    "            if online_plot:\n",
    "                di = self.train_config.device_iterations\n",
    "                ax.plot(np.arange(0,len(training_loss)*di,di),training_loss, c=colour)\n",
    "                ax.set(xlabel='Steps', ylabel='Training loss',\n",
    "                       title='Small Language Model trained on TinyStories')\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(plt.gcf())\n",
    "        \n",
    "        if online_plot:\n",
    "            display.clear_output(wait=True)\n",
    "        \n",
    "        return training_loss\n",
    "            \n",
    "    def validate(self):\n",
    "        torch.manual_seed(2937852)\n",
    "        valid_batches = islice(\n",
    "            self.dataset.batches(self.model.config.seq_length, 64, \"validation\"), 32\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            self.validation_loss = float(\n",
    "                torch.mean(torch.stack(list(map(self.model, valid_batches))))\n",
    "            )\n",
    "        if experiment.wandb:\n",
    "            wandb.log(dict(validation_loss=self.validation_loss), step=self.host_steps)\n",
    "            \n",
    "    def run(self, validation: bool, online_plot: bool):\n",
    "        self.training_loss = self.train(online_plot)\n",
    "        self.model.float()\n",
    "        self.model.train(False)\n",
    "        if validation:\n",
    "            self.validate()\n",
    "            print(f\"Validation loss: {self.validation_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we specify the run configurations of the model that we want to train, that determine the shapes of model components and its overall size. All configurations below are taken from the TinyStories papers and optimized for throughput on IPU. Additional configurations can be defined manually as well, although it's recommended not to deviate too far from the specified proportions below to avoid Out of Memory error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['hidden_dim', 'depth', 'pipeline_stages', 'compute_batch_size', 'offloading', 'checkpointing'] \n",
    "\n",
    "data = [\n",
    "    [64, 1, 1, 4, False, False],\n",
    "    [64, 2, 1, 4, False, False],\n",
    "    [64, 4, 1, 4, False, False],\n",
    "    [64, 8, 1, 4, False, False],\n",
    "    [64, 12, 1, 4, False, False],\n",
    "    [128, 1, 1, 4, False, False],\n",
    "    [128, 2, 1, 4, False, False],\n",
    "    [128, 4, 1, 4, False, False],\n",
    "    [128, 8, 1, 4, False, False],\n",
    "    [128, 12, 1, 4, False, False],\n",
    "    [256, 1, 1, 4, False, False],\n",
    "    [256, 2, 1, 4, False, False],\n",
    "    [256, 4, 1, 4, False, False],\n",
    "    [256, 8, 2, 4, False, True],\n",
    "    [256, 12, 2, 4, False, True],\n",
    "    [512, 1, 2, 2, False, True],\n",
    "    [512, 2, 2, 2, False, True],\n",
    "    [512, 4, 2, 2, False, True],\n",
    "    [512, 8, 2, 2, False, True],\n",
    "    [512, 12, 2, 2, False, True],\n",
    "    [768, 1, 2, 2, False, False],\n",
    "    [768, 2, 2, 2, False, True],\n",
    "    [768, 4, 4, 2, False, True],\n",
    "    [768, 8, 4, 2, False, True],\n",
    "    [768, 12, 4, 2, True, True],\n",
    "    [1024, 1, 2, 2, False, False],\n",
    "    [1024, 2, 2, 1, False, True],\n",
    "    [1024, 4, 4, 1, False, True],\n",
    "    [1024, 8, 4, 1, True, True],\n",
    "    [1024, 12, 4, 1, True, True],\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "run_configs = pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR Single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from IPython import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def hyperparameters(hidden_dim, depth, pipeline_stages, compute_batch_size, offloading, checkpointing):\n",
    "    return [int(hidden_dim), int(depth), int(pipeline_stages), int(compute_batch_size), offloading, checkpointing]\n",
    "\n",
    "if 'interactive_data' not in globals():\n",
    "    hd, depth, ps, cbs,offloading,checkpointing = '256', '8', 4, 2, False, True\n",
    "else:\n",
    "    hd, depth, ps, cbs,offloading,checkpointing = interactive_data.result\n",
    "    hd = str(hd)\n",
    "    depth = str(depth)\n",
    "    \n",
    "\n",
    "interactive_data = interactive(hyperparameters, \n",
    "                          hidden_dim=widgets.Dropdown(options=['64', '128', '256','512', '768','1024'], layout={'description_width': '1pt'}, \n",
    "                                                      value=hd, description='Hidden dim:', disabled=False),\n",
    "                          depth=widgets.Dropdown(options=['1', '2', '4','8', '12'], \n",
    "                                                      value=depth, description='Depth:', disabled=False), \n",
    "                          pipeline_stages=widgets.IntSlider(value=ps, min=1, max=4,step=1, \n",
    "                                                            description='Pipeline:',disabled=False), \n",
    "                          compute_batch_size=widgets.IntSlider(value=cbs, min=1, max=16,step=1, \n",
    "                                                            description='Batch size:',disabled=False), \n",
    "                          offloading=offloading, \n",
    "                          checkpointing=checkpointing)\n",
    "\n",
    "display.display(interactive_data)\n",
    "run_configs = pd.DataFrame([interactive_data.result], columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.create(vocab_size=8192, path=Path(\"data\"))\n",
    "\n",
    "for _, run_config in  run_configs.iterrows():\n",
    "    hidden_size, depth, pipeline_stages, compute_batch_size, offloading, checkpointing = run_config\n",
    "    \n",
    "    name =  f\"h{hidden_size}_l{depth}_model\"\n",
    "    experiment =  Experiment(\n",
    "        name,\n",
    "        ModelConfig(hidden_size=hidden_size, depth=depth, seq_length=512, head_size=64, \n",
    "                    dtype=\"half\", pipeline_stages=pipeline_stages, checkpointing=checkpointing),\n",
    "        TrainingConfig(lr=3e-4, steps=int(20000), batch_size=128, compute_batch_size=compute_batch_size, \n",
    "                        replicas=NUMBER_OF_IPUS//pipeline_stages, offloading=offloading,\n",
    "                       device_iterations=100, generation_temperature=0.5),\n",
    "        profiling=False,\n",
    "        wandb=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(experiment, dataset)\n",
    "    trainer.summary()\n",
    "    \n",
    "    try:\n",
    "        trainer.run(validation=True, online_plot=True)\n",
    "    finally:\n",
    "        trainer.poptorch_trainer.destroy()\n",
    "        \n",
    "    if MODEL_SAVING:\n",
    "        os.makedirs('checkpoints',exist_ok=True)\n",
    "        torch.save(trainer.model.state_dict(), os.path.join('checkpoints',name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PROMPTS = [\n",
    "    \"</s>\",\n",
    "    \"When Sally woke up, she\",\n",
    "    \"When I grow up, I\",\n",
    "    \"This was a sunny day\",\n",
    "    \"\"\"Once upon a time there was a little girl named Lucy. She was very adventurous.\n",
    "She loved to explore the world around her, especially when it was bright and sunny outside.\n",
    "One day, while exploring the nearby park, Lucy came across a ladder leaning on a wall.\n",
    "She was curious to see what's on top, so she climbed the ladder, but when she reached the top, the ladder fell and she was stuck.\n",
    "A nearby park ranger noticed her and shouted out \\\"\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generations = [\n",
    "                (\n",
    "                    prompt,\n",
    "                    trainer.model.generate(\n",
    "                        prompt, 150, experiment.train.generation_temperature\n",
    "                    ),\n",
    "                )\n",
    "                for prompt in TEST_PROMPTS\n",
    "            ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny stories nethome",
   "language": "python",
   "name": "tinystories_nethome"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
