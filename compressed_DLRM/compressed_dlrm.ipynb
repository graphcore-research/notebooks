{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressing DLRM\n",
    "\n",
    "Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Deep-learning recommendation models** (DLRMs) account for a vast portion of industry-scale ML workloads, from ranking relevant posts in news and social network feeds (comprising ~80% of AI inference cycles at Meta [[Gupta *et al.*, 2020](https://www.computer.org/csdl/proceedings-article/hpca/2020/614900a488/1j9wuchShGM)]) to making user-tailored suggestions on streaming platforms and e-commerce websites (driving up to 35% of Amazon's revenue in 2018 [[McKinsey](https://www.mckinsey.com/industries/retail/our-insights/how-retailers-can-keep-up-with-consumers)]). One key challenge of such models is the need of encoding sparse features (for example research keywords, product brands or user demographics), which — for the largest datasets — can assume hundreds of millions, if not billions, of different categorical values [[Ardalani *et al.*, 2022](https://arxiv.org/abs/2208.08489)]. The standard way of representing them is to learn a dense **embedding** for each categorical value [[Naumov *et al.*, 2019](https://arxiv.org/abs/1906.00091)], giving rise to huge, sparsely-accessed embedding tables — whose size can easily surpass the tens of terabytes. The sheer size of these tables poses significant engineering challenges and costs when coming up with solutions to efficiently train and deploy large DLRMs on AI accelerators [[Gupta *et al.*, 2020](https://www.computer.org/csdl/proceedings-article/hpca/2020/614900a488/1j9wuchShGM)], [[Mudigere *et al.*, 2022](https://dl.acm.org/doi/abs/10.1145/3470496.3533727)], typically requiring model distribution across multiple nodes just to be able to fit all parameters and complex memory management to distribute and communicate embeddings across a hierarchical sequence of storage supports (on-chip memory, host DRAM and SSDs) based on access frequency (see for instance Meta's hardware-software co-designed platform, ZionEx [[Mudigere *et al.*, 2022](https://dl.acm.org/doi/abs/10.1145/3470496.3533727)], dedicated to DLRM workloads).\n",
    "\n",
    "While the size of production-level DLRMs has steadily kept growing over the years [[Mudigere *et al.*, 2022](https://dl.acm.org/doi/abs/10.1145/3470496.3533727)], recent evidence [[Ardalani *et al.*, 2022](https://arxiv.org/abs/2208.08489)] suggests that — differently from Large Language Models — the canonical DLRM architectures are already significantly over-parametrized, operating way past the point of diminishing returns of their scaling law. As a consequence, a new interest in **compression techniques** for DLRMs has arisen [[Kang *et al.*, 2021](https://dl.acm.org/doi/10.1145/3447548.3467304)], [[Desai *et al.*, 2022](https://proceedings.neurips.cc/paper_files/paper/2022/hash/dbae915128892556134f1c5375855590-Abstract-Conference.html)], [[Hsia *et al.*, 2023](https://dl.acm.org/doi/10.1145/3582016.3582068)], investigating the possibility of **trading-off memory for compute** by generating embeddings on-the-fly when needed from a smaller set of trainable parameters, instead of having to store all of them all the time. In this notebook we will present some of these compression techniques and apply them to a basic DLRM architecture, analyzing their impact. As a study case we will run experiments on the [*Criteo 1TB Click Logs*](https://ailab.criteo.com/criteo-1tb-click-logs-dataset/) dataset for click-through-rate (CTR) prediction, commonly used for the recommendation benchmark of [MLPerf](https://mlcommons.org/en/training-normal-21/).\n",
    "\n",
    "Disclaimer: **this is a research notebook**, hence the provided implementation (similarly to the code in Meta's DLRM [repository](https://github.com/facebookresearch/dlrm/tree/main), upon which we base ourselves) is meant to be used as proof of concept and is not in any way optimized for throughput. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "The best way to run this demo is on Paperspace Gradient's cloud IPUs because everything is already set up for you.\n",
    "\n",
    " [![Run on Gradient](https://assets.paperspace.io/img/gradient-badge.svg)]()     TODO: ADD LINK\n",
    "\n",
    "To run the demo using other IPU hardware, you need to have the Poplar SDK enabled and a PopTorch wheel installed. Refer to the [Getting Started guide](https://docs.graphcore.ai/en/latest/getting-started.html#getting-started) for your system for details on how to do this. Also refer to the [Jupyter Quick Start guide](https://docs.graphcore.ai/projects/jupyter-notebook-quick-start/en/latest/index.html) for how to set up Jupyter to be able to run this notebook on a remote IPU machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "Let us install and import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy wandb scikit-learn intel_extension_for_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import numpy as np\n",
    "import poptorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import CriteoBinDataset, WandBLogger\n",
    "\n",
    "try:\n",
    "    # CPU multi-thread roc_auc_score implementation\n",
    "    from intel_extension_for_pytorch._C import roc_auc_score\n",
    "except:\n",
    "    from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment variables\n",
    "os.environ[\n",
    "    \"DATASET_DIR\"\n",
    "] = \"/net/group/research/criteo_datasets/criteo_terabyte/\"  # TODO: REMOVE WITH RELEASE\n",
    "os.environ[\"CHECKPOINT_DIR\"] = \"./checkpoints\"  # TODO: REMOVE WITH RELEASE\n",
    "\n",
    "dataset_dir = Path(os.getenv(\"DATASET_DIR\"))\n",
    "checkpoint_dir = Path(os.getenv(\"CHECKPOINT_DIR\"))\n",
    "NUM_AVAILABLE_IPUS = int(os.getenv(\"NUM_AVAILABLE_IPU\", 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell downloads the preprocessed *Criteo 1TB* dataset. Notice that the training dataset is sizeable (~625 GB): we recommend downloading it (by uncommenting the corresponding line in the cell) only if the user is interested in running training firsthand. We advise against doing so on machines with less than 16 IPUs.\n",
    "\n",
    "We also download checkpoints of trained models that can be used to run inference and compare the final performance on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./checkpoints/compressed_dlrm_checkpoints.zip\n",
      "  inflating: ./checkpoints/compemb_adamsgd.pt  \n",
      "  inflating: ./checkpoints/compemb_sgd.pt  \n",
      "  inflating: ./checkpoints/dhe_sgd.pt  \n",
      "  inflating: ./checkpoints/ht_sgd.pt  \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# download test dataset TODO: uncomment\n",
    "# aws s3 --no-sign-request cp s3://graphcore-research-public/2023-dlrm-notebook/day_fea_count.npz $DATASET_DIR/day_fea_count.npz\n",
    "# aws s3 --no-sign-request cp s3://graphcore-research-public/2023-dlrm-notebook/terabyte_processed_val.bin $DATASET_DIR/terabyte_processed_val.bin\n",
    "\n",
    "# uncomment the following line to download the train dataset\n",
    "# aws s3 --no-sign-request cp s3://graphcore-research-public/2023-dlrm-notebook/terabyte_processed_train.bin $DATASET_DIR/terabyte_processed_train.bin\n",
    "\n",
    "# download model checkpoints\n",
    "wget -q -P $CHECKPOINT_DIR/ https://graphcore-research-public.s3.eu-west-1.amazonaws.com/2023-dlrm-notebook/compressed_dlrm_checkpoints.zip\n",
    "unzip $CHECKPOINT_DIR/compressed_dlrm_checkpoints.zip -d $CHECKPOINT_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DLRM model\n",
    "\n",
    "We base our recommender model on the public-source DLRM [[Naumov *et al.*, 2019](https://arxiv.org/abs/1906.00091)] implementation by Meta ([github repository](https://github.com/facebookresearch/dlrm/tree/main)). The model's inputs are divided between dense features and sparse features: we represent the former as a floating point vector $\\textbf{y} \\in \\mathbb{R}^d$, while for each sparse feature the input consists of the integer ID of the corresponding categorical value. For the sake of simplicity (and because this is the case in the *Criteo 1TB* dataset) in the following we will assume that each datapoint encodes a single categorical value for each sparse feature, hence the categorical inputs can be represented by a vector $\\textbf{x} \\in \\mathbb{N}^s$, where $s$ is the number of sparse features. More generally, multiple categorical values may need to be combined for each sparse feature, and everything we will present can be easily extended to this case.   \n",
    "\n",
    "The standard DLRM implementation uses **embedding operators** to map each categorical value to a dense representation. If $n_k$ is the number of categories for the $k$-th sparse feature, the embedding of its $i$-th categorical value is given by the $i$-th row of a trainable embedding table $E_k \\in \\mathbb{R}^{n_k,e}$, with $e$ being the embedding size. Starting from the categorical input $\\textbf{x} \\in \\mathbb{N}^s$ we therefore look-up the $s$ embeddings $\\textbf{e}_1, \\dots, \\textbf{e}_s \\in \\mathbb{R}^e$, where $\\textbf{e}_k$ is the $x_k$-th row of $E_k$. An additional $e$-dimensional vector is generated from the dense input $\\textbf{y}$ by passing it through an MLP stack (called **bottom MLP**), namely $\\textbf{e}_0 := \\text{MLP}_{\\text{bot}}(\\textbf{y}) \\in \\mathbb{R}^e$. The $s+1$ vectors $\\textbf{e}_0, \\dots, \\textbf{e}_s$ are then combined through an **interaction operator**: this could be chosen to be simple concatenation $\\text{int}(\\textbf{e}_0, \\dots, \\textbf{e}_s) := \\text{concat}(\\textbf{e}_0, \\dots, \\textbf{e}_s) \\in \\mathbb{R}^{(s+1)e}$, or a more complex operation like computing all distinct pairwise dot products $\\text{int}(\\textbf{e}_0, \\dots, \\textbf{e}_s) := (\\textbf{e}_i \\cdot \\textbf{e}_j)_{i < j} \\in \\mathbb{R}^{s+1 \\choose 2}$ (in practice, this vector is then usually concatenated with $\\textbf{e}_0$). The result of the interaction operator goes through a final MLP stack (called **top MLP**) with 1-dimensional output $l = \\text{MLP}_{\\text{top}}(\\text{int}(\\textbf{e}_0, \\dots, \\textbf{e}_s)) \\in \\mathbb{R}$, which is interpreted as the logit of the click-through-rate probability.\n",
    "\n",
    "<img src=\"img/DLRM.jpg\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/facebookresearch/dlrm/blob/main/dlrm_s_pytorch.py\n",
    "\n",
    "\n",
    "class DLRM(nn.Module):\n",
    "    def create_mlp(self, layers_size, sigmoid_layer):\n",
    "        layers = nn.ModuleList()\n",
    "        for i in range(len(layers_size) - 1):\n",
    "            in_dim, out_dim = layers_size[i], layers_size[i + 1]\n",
    "\n",
    "            LL = nn.Linear(int(in_dim), int(out_dim), bias=True)\n",
    "            mean = 0.0\n",
    "            std_dev_w = np.sqrt(2 / (in_dim + out_dim))\n",
    "            W = np.random.normal(mean, std_dev_w, size=(out_dim, in_dim)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "            std_dev_b = np.sqrt(1 / out_dim)\n",
    "            b = np.random.normal(mean, std_dev_b, size=out_dim).astype(np.float32)\n",
    "            LL.weight.data = torch.tensor(W, requires_grad=True)\n",
    "            LL.bias.data = torch.tensor(b, requires_grad=True)\n",
    "            layers.append(LL)\n",
    "\n",
    "            if i == sigmoid_layer:\n",
    "                layers.append(nn.Sigmoid())\n",
    "            else:\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "    def create_emb(self, emb_dim, tables_size):\n",
    "        emb_l = nn.ModuleList()\n",
    "        for t_size in tables_size:\n",
    "            EE = nn.Embedding(t_size, emb_dim)\n",
    "            W = np.random.uniform(\n",
    "                low=-np.sqrt(1 / t_size),\n",
    "                high=np.sqrt(1 / t_size),\n",
    "                size=(t_size, emb_dim),\n",
    "            ).astype(np.float32)\n",
    "            EE.weight.data = torch.tensor(W, requires_grad=True)\n",
    "            emb_l.append(EE)\n",
    "        return emb_l\n",
    "\n",
    "    @property\n",
    "    def n_emb_parameters(self) -> int:\n",
    "        return sum([x.numel() for x in self.emb_list.parameters()])\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dim: int,\n",
    "        emb_table_sizes: list,\n",
    "        mlp_layers_size_bot: str,\n",
    "        mlp_layers_size_top: str,\n",
    "        arch_interaction_op: str,\n",
    "        arch_interaction_itself: bool = False,\n",
    "        sigmoid_bot: int = -1,\n",
    "        sigmoid_top: int = -1,\n",
    "        loss_threshold: float = 0.0,\n",
    "        loss_function: str = \"bce\",\n",
    "        loss_weights: str = None,\n",
    "    ):\n",
    "        super(DLRM, self).__init__()\n",
    "        self.arch_interaction_op = arch_interaction_op\n",
    "        self.arch_interaction_itself = arch_interaction_itself\n",
    "        self.loss_threshold = loss_threshold\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "        # Bottom and top MLP stacks\n",
    "        self.bot_mlp = self.create_mlp(mlp_layers_size_bot, sigmoid_bot)\n",
    "        self.top_mlp = self.create_mlp(mlp_layers_size_top, sigmoid_top)\n",
    "\n",
    "        # List of embedding operators\n",
    "        self.emb_list = self.create_emb(emb_dim, emb_table_sizes)\n",
    "\n",
    "        # Loss function\n",
    "        if self.loss_function == \"mse\":\n",
    "            self.loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "        elif self.loss_function == \"bce\":\n",
    "            self.loss_fn = torch.nn.BCELoss(reduction=\"mean\")\n",
    "        elif self.loss_function == \"wbce\":\n",
    "            self.register_buffer(\n",
    "                \"loss_ws\",\n",
    "                torch.tensor(np.fromstring(loss_weights, dtype=float, sep=\"-\")),\n",
    "            )\n",
    "            self.loss_fn = torch.nn.BCELoss(reduction=\"none\")\n",
    "        else:\n",
    "            sys.exit(\n",
    "                \"ERROR: --loss-function=\" + self.loss_function + \" is not supported\"\n",
    "            )\n",
    "\n",
    "    def apply_mlp(self, x, layers):\n",
    "        return layers(x)\n",
    "\n",
    "    def apply_emb(self, lookup_ids):\n",
    "        ly = []\n",
    "        for k, t in enumerate(self.emb_list):\n",
    "            ly.append(t(lookup_ids[:, k]))\n",
    "        out = torch.stack(ly, dim=1)  # shape (bs, n_sparse_features, emb_dim)\n",
    "        return out\n",
    "\n",
    "    def interact_features(self, dense_feat, sparse_feat):\n",
    "        if self.arch_interaction_op == \"dot\":\n",
    "            # concatenate dense and sparse features\n",
    "            T = torch.cat(\n",
    "                [dense_feat.unsqueeze(1), sparse_feat], dim=1\n",
    "            )  # shape (bs, 1 + n_sparse_features, emb_dim)\n",
    "            # perform a dot product\n",
    "            Z = torch.bmm(T, torch.transpose(T, 1, 2))\n",
    "            _, ni, nj = Z.shape\n",
    "            offset = 1 if self.arch_interaction_itself else 0\n",
    "            li = torch.tensor(\n",
    "                [i for i in range(ni) for j in range(i + offset)],\n",
    "                device=dense_feat.device,\n",
    "                dtype=torch.int32,\n",
    "            )\n",
    "            lj = torch.tensor(\n",
    "                [j for i in range(nj) for j in range(i + offset)],\n",
    "                device=dense_feat.device,\n",
    "                dtype=torch.int32,\n",
    "            )\n",
    "            Zflat = Z[:, li, lj]\n",
    "            # final output is concatenation of dense features and pairwise interactions\n",
    "            R = torch.cat([dense_feat] + [Zflat], dim=1)\n",
    "        elif self.arch_interaction_op == \"cat\":\n",
    "            # concatenation features (into a row vector)\n",
    "            R = torch.cat(\n",
    "                [dense_feat, sparse_feat.view(sparse_feat.shape[0], -1)], dim=1\n",
    "            )\n",
    "        else:\n",
    "            sys.exit(\n",
    "                \"ERROR: --arch-interaction-op=\"\n",
    "                + self.arch_interaction_op\n",
    "                + \" is not supported\"\n",
    "            )\n",
    "\n",
    "        return R\n",
    "\n",
    "    def forward(self, dense_x, lS_i, labels=None):\n",
    "        \"\"\"\n",
    "        dense_x: shape (bs, n_dense_features)\n",
    "            Values of dense features.\n",
    "        lS_i: shape (bs, n_sparse_features)\n",
    "            Categorical value IDs of sparse features.\n",
    "        labels (optional): shape (bs, 1)\n",
    "            CTR labels (0/1).\n",
    "\n",
    "        Returns:\n",
    "            CTR probability: shape (bs, 1)\n",
    "            Reduced batch loss (if labels are provided): shape (,)\n",
    "        \"\"\"\n",
    "        # requirement for poptorch dataloader\n",
    "        dense_x = dense_x.squeeze(0)\n",
    "        lS_i = lS_i.squeeze(0)\n",
    "        if labels is not None:\n",
    "            labels = labels.squeeze(0)\n",
    "\n",
    "        # process dense features (using bottom mlp)\n",
    "        dense_feat = self.apply_mlp(dense_x, self.bot_mlp)\n",
    "\n",
    "        # process sparse features (using embedding operators)\n",
    "        sparse_feat = self.apply_emb(lS_i)\n",
    "\n",
    "        # interact features (dense and sparse)\n",
    "        z = self.interact_features(dense_feat, sparse_feat)\n",
    "\n",
    "        # obtain probability of a click (using top mlp)\n",
    "        out = self.apply_mlp(z, self.top_mlp)\n",
    "\n",
    "        # clamp output if needed\n",
    "        if 0.0 < self.loss_threshold and self.loss_threshold < 1.0:\n",
    "            out = torch.clamp(\n",
    "                out, min=self.loss_threshold, max=(1.0 - self.loss_threshold)\n",
    "            )\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(out, labels)\n",
    "            if self.loss_function == \"wbce\":\n",
    "                loss_ws_ = self.loss_ws[labels.data.view(-1)].view_as(labels)\n",
    "                loss = poptorch.identity_loss(loss_ws_ * loss, reduction=\"sum\")\n",
    "            out = [out, loss]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding compression\n",
    "\n",
    "The total number of trainable parameters in DLRM is given by:  `#params_MLP_bot + #params_MLP_top + #params_embedding`, with `#params_embedding = e*(n_1+...+n_s)` typically accounting for the vast majority of the total [[Ardalani *et al.*, 2022](https://arxiv.org/abs/2208.08489)]. Let us look for instance at the *Criteo 1TB* dataset, taking the sizes for embeddings and MLP layers as in the [reference MLPerf configuration](https://github.com/facebookresearch/dlrm/blob/main/bench/run_and_time.sh)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sparse features: 26\n",
      "Number of categorical values per feature: [227605432, 39060, 17295, 7424, 20265, 3, 7122, 1543, 63, 130229467, 3067956, 405282, 10, 2209, 11938, 155, 4, 976, 14, 292775614, 40790948, 187188510, 590152, 12973, 108, 36]\n",
      "Total number of categories: 882,774,559\n",
      "Embedding size: 128\n",
      "Total number of embedding parameters: 112,995,143,552\n",
      "Total size of embedding parameters (fp32): 451.98 GB\n"
     ]
    }
   ],
   "source": [
    "# Categorical value counts per sparse feature\n",
    "with np.load(dataset_dir.joinpath(\"day_fea_count.npz\")) as data:\n",
    "    counts = data[\"counts\"]\n",
    "\n",
    "n_byte_per_parameter = {\"fp32\": 4, \"fp16\": 2}\n",
    "\n",
    "n_sparse = counts.shape[0]\n",
    "emb_size = 128\n",
    "dtype = \"fp32\"\n",
    "\n",
    "print(f\"Number of sparse features: {n_sparse}\")\n",
    "print(f\"Number of categorical values per feature: {counts.tolist()}\")\n",
    "print(f\"Total number of categories: {counts.sum():,}\")\n",
    "print(f\"Embedding size: {emb_size:,}\")\n",
    "print(f\"Total number of embedding parameters: {emb_size * counts.sum():,}\")\n",
    "print(\n",
    "    f\"Total size of embedding parameters ({dtype}): {n_byte_per_parameter[dtype] * emb_size * counts.sum() / 1e9:.5} GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dense features: 13\n",
      "Number of bottom MLP parameters: 171,392\n",
      "Number of top MLP parameters: 2,197,505\n",
      "Total size of MLP parameters (fp32): 0.0094756 GB\n"
     ]
    }
   ],
   "source": [
    "n_dense = 13\n",
    "mlp_bot_layers = [n_dense, 512, 256, emb_size]\n",
    "# Assume dot product interaction without self-interactions\n",
    "n_interactions = n_sparse * (n_sparse + 1) // 2\n",
    "mlp_top_layers = [emb_size + n_interactions] + [1024, 1024, 512, 256, 1]\n",
    "\n",
    "# Sum of parameters (weights and biases) over all linear layers\n",
    "mlp_bot_params = sum(\n",
    "    [\n",
    "        (mlp_bot_layers[i] + 1) * mlp_bot_layers[i + 1]\n",
    "        for i in range(len(mlp_bot_layers) - 1)\n",
    "    ]\n",
    ")\n",
    "mlp_top_params = sum(\n",
    "    [\n",
    "        (mlp_top_layers[i] + 1) * mlp_top_layers[i + 1]\n",
    "        for i in range(len(mlp_top_layers) - 1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Number of dense features: {n_dense}\")\n",
    "print(f\"Number of bottom MLP parameters: {mlp_bot_params:,}\")\n",
    "print(f\"Number of top MLP parameters: {mlp_top_params:,}\")\n",
    "print(\n",
    "    f\"Total size of MLP parameters ({dtype}): {n_byte_per_parameter[dtype] * (mlp_bot_params + mlp_top_params) / 1e9:.5} GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, **embedding parameters account for > 99.99% of all parameters in the model**, requiring almost half a TB of memory to be stored! Embedding compression techniques aim to reduce the size of embedding parameters. A basic form of compression, known as **Hashing Trick** [[Weinberger *et al.*, 2009](https://dl.acm.org/doi/10.1145/1553374.1553516)], is already present in the DLRM source code: it consists of vertically cropping the largest embedding tables, by imposing a maximum number of rows per table. Categorical values' IDs are then hashed to this smaller range: as a consequence, some embeddings will be shared among multiple categorical values. If $M$ is the maximum number of rows allowed, then the embedding for the $i$-th categorical value is retrieved by looking-up the $(i \\mod M)$-th row of the table. This is explicitly used in the [MLPerf reference configuration](https://github.com/facebookresearch/dlrm/blob/main/bench/run_and_time.sh) for *Criteo 1TB*, with `M = 4e7`, resulting in a reduction of the number of embedding parameters of approximately 4x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashing Trick:\n",
      "Number of rows in cropped embedding tables: [40000000, 39060, 17295, 7424, 20265, 3, 7122, 1543, 63, 40000000, 3067956, 405282, 10, 2209, 11938, 155, 4, 976, 14, 40000000, 40000000, 40000000, 590152, 12973, 108, 36]\n",
      "Total number of unique embeddings: 204,184,588\n",
      "Embedding size: 128\n",
      "Total number of embedding parameters: 26,135,627,264\n",
      "Total size of embedding parameters (fp32): 104.54 GB\n"
     ]
    }
   ],
   "source": [
    "max_ind_range = 40000000  # M\n",
    "ht_counts = np.minimum(counts, max_ind_range)\n",
    "\n",
    "print(\"Hashing Trick:\")\n",
    "print(f\"Number of rows in cropped embedding tables: {ht_counts.tolist()}\")\n",
    "print(f\"Total number of unique embeddings: {ht_counts.sum():,}\")\n",
    "print(f\"Embedding size: {emb_size:,}\")\n",
    "print(f\"Total number of embedding parameters: {emb_size * ht_counts.sum():,}\")\n",
    "print(\n",
    "    f\"Total size of embedding parameters ({dtype}): {n_byte_per_parameter[dtype] * emb_size * ht_counts.sum() / 1e9:.5} GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashing Trick forces multiple categorical values to have the same exact embedding, potentially limiting the model's expressivity. We now present two more advanced compression techniques which will allow us to overcome this limitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compositional Embedding\n",
    "\n",
    "The first embedding compression technique, which we will call **Compositional Embedding**, is inspired by the ROBE-Z papers [[Desai *et al.*, 2021](https://arxiv.org/abs/2108.02191)], [[Desai *et al.*, 2022](https://proceedings.neurips.cc/paper_files/paper/2022/hash/dbae915128892556134f1c5375855590-Abstract-Conference.html)], in which the authors report being able to achieve a compression factor of **10000x** for *Criteo 1TB*.\n",
    "\n",
    "The key idea is to generate embeddings by concatenating smaller chunks, extracted from a parameter array (shared among all tables) of arbitrary size. Differently from [[Desai *et al.*, 2021](https://arxiv.org/abs/2108.02191)] where the starting point of each chunk in the parameter array can be completely arbitrary, here we use non-overlapping chunks to improve look-up efficiency: while this reduces, in principle, the model's expressivity, we do not see significant differences in the final performance.\n",
    "\n",
    "More precisely, ROBE-Z replaces the embedding operators with a 1D array of trainable parameters $W \\in \\mathbb{R}^N$, whose size $N$ is an hyperparameter determining the compression rate. Let $e$ be the embedding size, $Z$ be the size of each chunk (which we assume divides $e$) and $R = e/Z$ be the number of chunks that are concatenated to generate each embedding. The starting positions of the chunks are determined by a universal hash function $ h : \\mathbb{N} \\times \\mathbb{N} \\times \\mathbb{N} \\rightarrow \\left\\{ 0, \\dots, N-Z-1 \\right\\} $ so that the embedding for the $x$-th categorical value of the $k$-th sparse feature is obtained as\n",
    "$$ \\text{concat}(C_0, C_1, \\dots, C_{R-1}) \\in \\mathbb{R}^{e}, \\quad \\text{with } C_j := W[h(x,k,j): h(x,k,j) + Z] \\in \\mathbb{R}^{Z}.$$\n",
    "\n",
    "For our Compositional Embedding, on the other hand, we use a 2D trainable tensor $W \\in \\mathbb{R}^{N,Z}$, where each row corresponds to a different chunk. Our hash function maps now to the 0-th axis of $W$, namely $ h : \\mathbb{N} \\times \\mathbb{N} \\times \\mathbb{N} \\rightarrow \\left\\{ 0, \\dots, M-1 \\right\\} $ and the embedding for the $x$-th categorical value of the $k$-th sparse feature is given by\n",
    "$$ \\text{concat}(\\tilde{C}_0, \\tilde{C}_1, \\dots, \\tilde{C}_{R-1}) \\in \\mathbb{R}^{e}, \\quad \\text{with } \\tilde{C}_j := W[h(x,k,j)] \\in \\mathbb{R}^{Z}.$$\n",
    "\n",
    "Notice that, by combining multiple chunks, the probability of having two categorical values with the same embedding is significantly reduced compared to Hashing Trick.\n",
    "\n",
    "Similarly to what is done in the ROBE-Z papers, we use a classical family of universal hash functions based on modular arithmetic:\n",
    "$$ h(x,k,j) = (Ax + Bk + Cj + D)  \\mod P \\mod M $$\n",
    "with $P$ a large prime number and $A,B,C,D$ random non-zero integers modulo $P$. \n",
    "\n",
    "To implement this, we only need to pay attention to the fact that on the IPU we cannot represent 64-bit integers: this is potentially an issue since the values $Ax + Bk + Cj + D$, if computed naively, are likely to overflow in `int32`. To overcome this obstacle, notice that the Compositional Embedding module will take an input $X$ of shape `(bs, n_sparse_features)`, containing the categorical values $x$ for each sparse feature. One can therefore pre-compute the batch-independent tensor $T = \\left( (Bk + Cj + D) \\mod P \\right)_{\\substack{k=0,\\dots,n\\_sparse, \\\\ j=0,\\dots,R-1}}$ of size `(1, n_sparse_features, R)` and modify the dataloader so that it computes $AX$ (which still has shape `(bs, n_table)`) on host in `int64` before sending its remainder mod $P$ to the device (as `int32`). The final `(bs, n_sparse_features, R)` table of hashed values can be then obtained by broadcasting as $$((AX \\mod P) + T) \\mod P \\mod M$$ \n",
    "which, as long as we take $P < \\text{max(int32)} / 2$, can be computed in `int32` with no overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositionalEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dim: int,\n",
    "        table_sizes: list,\n",
    "        chunk_size: int,\n",
    "        m: int,\n",
    "        p: int = 1000000007,\n",
    "    ) -> None:\n",
    "        super(CompositionalEmbedding, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.table_offset = torch.cumsum(torch.tensor([0] + table_sizes[:-1]), dim=0)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.M = m\n",
    "        self.register_buffer(\"P\", torch.tensor([p], dtype=torch.int32))\n",
    "        self.register_buffer(\n",
    "            \"random_numbers\",\n",
    "            torch.from_numpy(\n",
    "                np.concatenate([np.array([p]), np.random.randint(1, p, (3,))])\n",
    "            ).contiguous(),\n",
    "        )\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.from_numpy(\n",
    "                np.random.uniform(\n",
    "                    low=-np.sqrt(1 / max(table_sizes)),\n",
    "                    high=np.sqrt(1 / max(table_sizes)),\n",
    "                    size=((self.M, self.chunk_size)),\n",
    "                )\n",
    "            )\n",
    "            .contiguous()\n",
    "            .to(torch.float32)\n",
    "        )\n",
    "\n",
    "        chunk_id = torch.arange(self.emb_dim // self.chunk_size)\n",
    "        self.register_buffer(\n",
    "            \"hash_buffer\",\n",
    "            (\n",
    "                self.table_offset.view(-1, 1) * self.random_numbers[3]\n",
    "                + chunk_id * self.random_numbers[2]\n",
    "                + self.random_numbers[1]\n",
    "            )\n",
    "            .fmod(self.P)\n",
    "            .contiguous()\n",
    "            .to(torch.int32),\n",
    "        )  # shape (n_table, n_chunk)\n",
    "\n",
    "        print(\n",
    "            \"Compositional Embedding: dim:{} weight_size:{} chunk_size:{}\".format(\n",
    "                self.emb_dim,\n",
    "                self.weight.shape,\n",
    "                self.chunk_size,\n",
    "            ),\n",
    "            flush=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, indices: torch.Tensor) -> torch.Tensor:\n",
    "        # Hash indices\n",
    "        hashed_idx = (\n",
    "            (indices.unsqueeze(-1) + self.hash_buffer).fmod(self.P).fmod(self.M)\n",
    "        )  # shape (bs, n_table, n_chunk)\n",
    "\n",
    "        # Chunk lookup and concatenation\n",
    "        out = self.weight[hashed_idx.detach()].flatten(\n",
    "            start_dim=-2\n",
    "        )  # shape (bs, n_table, emb_dim)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Hash Embedding\n",
    "\n",
    "The second embedding compression method that we want to test is Deep Hash Embedding (DHE), introduced in [[Kang *et al.*, 2021](https://dl.acm.org/doi/10.1145/3447548.3467304)] and applied in [[Hsia *et al.*, 2023](https://dl.acm.org/doi/10.1145/3582016.3582068)] to the *Criteo 1TB* dataset. Similarly to ROBE-Z, DHE also applies an hash function to the categorical value IDs; however, instead of using these hashed values to perform lookups, it passes them (possibly after some additional transformations) through an MLP stack, which outputs the final embedding vectors. Thus, DHE removes embedding tables entirely: its trainable parameters consist only of the weights in these additional fully connected layers.\n",
    "\n",
    "To be more specific, *for each sparse feature* in the uncompressed model we introduce $K$ hash functions $h_0, \\dots, h_{K-1}: \\mathbb{N} \\rightarrow \\left\\{0, \\dots, M-1 \\right\\}$. Then, to generate the $e$-dimensional embedding for the $x$-th categorical value we first compute the vector of integers $(h_0(x), \\dots, h_{K-1}(x)) \\in \\left\\{0, \\dots, M-1 \\right\\}^K$, normalize it and then pass it through an MLP stack $\\mathbb{R}^K \\rightarrow \\mathbb{R}^{d_1} \\rightarrow \\dots \\rightarrow \\mathbb{R}^{d_n} \\rightarrow \\mathbb{R}^e$.\n",
    "\n",
    "Here we use multiply-shift hashing [[Dietzfelbinger *et al.*, 1997](https://www.sciencedirect.com/science/article/abs/pii/S0196677497908737)], suitable to working in `int32`. Set $M = 2^l$ and define\n",
    "$$ h(x) = \\lfloor (Ax \\mod 2^{32}) / 2^{32-l} \\rfloor$$\n",
    "with $A$ a random odd integer. If `x` is a `uint32` (a format not supported in PyTorch), then this gives a 2-approximately universal family of hash functions with values in $\\left\\{0, \\dots, 2^l-1 \\right\\}$. Notice that the remainder $Ax \\mod 2^{32}$ is the result of the product $Ax$ when computed in 32-bit precision, and the floor division by $2^{32-l}$ can be performed with a simple bitwise right shift (from this, the name multiply-shift).\n",
    "\n",
    "However, as we need to use the signed `int32` format, the 32-bit product $Ax$ will provide (after possibly overflowing) a result in $\\left\\{-2^{31}, \\dots, 2^{31}-1 \\right\\}$, corresponding mathematically to $\\left((Ax + 2^{31}) \\mod 2^{32}\\right)  - 2^{31}$. Therefore $h(x) \\in \\left\\{-2^{l-1}, \\dots, 2^{l-1}-1 \\right\\}$. This is not a problem, as in any case the output of the hash function is normalized to $[-1, 1]$ before being passed on to the first MLP layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchedLinear(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Batched Linear layer: like torch.nn.Linear but applying different linear\n",
    "    transformations along a batched dimension, using torch.bmm for batched\n",
    "    matrix-matrix product.\n",
    "    Input: shape (batch_dim, *, in_features)\n",
    "    Weight: shape (batch_dim, in_features, out_features)\n",
    "    Bias: shape (batch_dim, 1, out_features)\n",
    "    Output: shape (batch_dim, *, out_features)\n",
    "    with out[i] = in[i] @ W[i] + b[i]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_dim, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.batch_dim = batch_dim\n",
    "        self.use_bias = bias\n",
    "        self.weight = torch.nn.Parameter(\n",
    "            torch.empty(batch_dim, in_features, out_features)\n",
    "        )\n",
    "        if self.use_bias:\n",
    "            self.bias = torch.nn.Parameter(torch.empty(batch_dim, 1, out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            torch.nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        batched_matmul = torch.bmm(input, self.weight)\n",
    "        if self.use_bias:\n",
    "            batched_matmul = torch.add(batched_matmul, self.bias)\n",
    "        return batched_matmul\n",
    "\n",
    "\n",
    "class BatchedBatchNorm(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Applies Batch Normalization separately along leftmost dimension.\n",
    "    Input: shape (b, batch_size, n_features)\n",
    "    Output: same shape as input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **args):\n",
    "        super().__init__()\n",
    "        self.b_norm = torch.nn.BatchNorm1d(**args)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        trans_input = torch.permute(input, (1, 2, 0))\n",
    "        return torch.permute(self.b_norm(trans_input), (2, 0, 1))\n",
    "\n",
    "\n",
    "class DHE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        table_sizes: list,\n",
    "        m_exp: int,  # M = 2**m_exp\n",
    "        n_hash_per_table: int,\n",
    "        mlp_dims: list,\n",
    "        mlp_activation: str,\n",
    "        batch_norm: bool,\n",
    "    ) -> None:\n",
    "        super(DHE, self).__init__()\n",
    "        self.mlp_dims = mlp_dims\n",
    "        self.n_hash_per_table = n_hash_per_table\n",
    "        self.m_exp_m1 = m_exp - 1\n",
    "        n_table = len(table_sizes)\n",
    "        self.register_buffer(\n",
    "            \"hash_a\",\n",
    "            torch.from_numpy(\n",
    "                2 * np.random.randint(1, 2**30, (n_table, 1, self.n_hash_per_table))\n",
    "                + 1\n",
    "            ).to(torch.int32),\n",
    "        )  # shape (n_table, 1, n_hash_per_table)\n",
    "\n",
    "        # DHE decoder (MLP)\n",
    "        layers = nn.ModuleList()\n",
    "        for i in range(len(mlp_dims) - 1):\n",
    "            dim_in = int(mlp_dims[i])\n",
    "            dim_out = int(mlp_dims[i + 1])\n",
    "            layer = BatchedLinear(n_table, dim_in, dim_out, bias=True)\n",
    "            mu = 0.0\n",
    "            sigma_w = np.sqrt(2 / (dim_in + dim_out))\n",
    "            sigma_b = np.sqrt(1 / dim_out)\n",
    "            w = np.random.normal(mu, sigma_w, size=(n_table, dim_in, dim_out)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "            b = np.random.normal(mu, sigma_b, size=(n_table, 1, dim_out)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "\n",
    "            layer.weight.data = torch.tensor(w, requires_grad=True)\n",
    "            layer.bias.data = torch.tensor(b, requires_grad=True)\n",
    "            layers.append(layer)\n",
    "\n",
    "            if i < len(mlp_dims) - 2:\n",
    "                layers.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                if mlp_activation == \"relu\":\n",
    "                    layers.append(torch.nn.ReLU())\n",
    "                elif mlp_activation == \"sigmoid\":\n",
    "                    layers.append(torch.nn.Sigmoid())\n",
    "                elif mlp_activation == \"mish\":\n",
    "                    layers.append(torch.nn.Mish())\n",
    "                else:\n",
    "                    sys.exit(\"Unsupported DHE MLP Activation Function\")\n",
    "\n",
    "                # Batch Normalization\n",
    "                if batch_norm:\n",
    "                    layers.append(BatchedBatchNorm(num_features=dim_out))\n",
    "\n",
    "            self.dhe_mlp = torch.nn.Sequential(*layers)\n",
    "\n",
    "        print(\n",
    "            \"DHE: dim:{} n_hash_per_table:{} mlp_dims:{}\".format(\n",
    "                self.mlp_dims[-1],\n",
    "                self.n_hash_per_table,\n",
    "                self.mlp_dims,\n",
    "            ),\n",
    "            flush=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, indices: torch.Tensor) -> torch.Tensor:\n",
    "        # Encoding (multiply-shift hashing)\n",
    "        hashed_idx = torch.floor(\n",
    "            torch.mul(indices.T.unsqueeze(-1), self.hash_a)\n",
    "            / torch.tensor(\n",
    "                [2 ** (31 - self.m_exp_m1)], dtype=torch.float32, device=indices.device\n",
    "            ),\n",
    "        )  # shape (n_table, bs, n_hash_per_table)\n",
    "        # Encoding (normalization)\n",
    "        hashed_idx = hashed_idx.to(torch.float32) / 2**self.m_exp_m1\n",
    "\n",
    "        # Decoding (MLP)\n",
    "        out = torch.transpose(\n",
    "            self.dhe_mlp(hashed_idx.detach()), 0, 1\n",
    "        )  # shape (bs, n_table, emb_dim)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the `CompressedDLRM` class, supporting either Compositional Embedding or DHE, by overriding the `create_emb` and `apply_emb` methods of the original `DLRM` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CompressionConfig:\n",
    "    # embedding compression techinque\n",
    "    compression_type: Literal[\"comp_emb\", \"dhe\"]\n",
    "    # size of Compositional Embedding chunks\n",
    "    comp_emb_chunk_size: int = 1\n",
    "    # number of Compositional Embedding trainable parameters\n",
    "    comp_emb_n_parameters: int = 10000000\n",
    "    # number of DHE hashing functions per table\n",
    "    dhe_n_hash: int = 1024\n",
    "    # DHE hash range = 2**dhe_m_exp\n",
    "    dhe_m_exp: int = 23\n",
    "    # DHE decoder MLP layer sizes\n",
    "    # (excluding initial dhe_n_hash dimension)\n",
    "    dhe_mlp_dims: str = \"512-256-128\"\n",
    "    # final activation layer for DHE decoder MLP\n",
    "    dhe_mlp_activation: Literal[\"relu\", \"sigmoid\", \"mish\"] = \"mish\"\n",
    "    # apply final batch norm to DHE decoder MLP\n",
    "    dhe_batch_norm: bool = False\n",
    "\n",
    "\n",
    "class CompressedDLRM(DLRM):\n",
    "    def create_emb(\n",
    "        self,\n",
    "        emb_dim: int,\n",
    "        emb_table_sizes: list,\n",
    "    ) -> Union[CompositionalEmbedding, DHE]:\n",
    "        compr_config = self.compression_config\n",
    "        if compr_config.compression_type == \"comp_emb\":\n",
    "            emb_l = CompositionalEmbedding(\n",
    "                emb_dim,\n",
    "                emb_table_sizes,\n",
    "                compr_config.comp_emb_chunk_size,\n",
    "                m=compr_config.comp_emb_n_parameters\n",
    "                // compr_config.comp_emb_chunk_size,\n",
    "            )\n",
    "        elif self.compression_config.compression_type == \"dhe\":\n",
    "            dhe_mlp_dims = np.fromstring(\n",
    "                f\"{compr_config.dhe_n_hash}-\" + compr_config.dhe_mlp_dims,\n",
    "                dtype=int,\n",
    "                sep=\"-\",\n",
    "            )\n",
    "            emb_l = DHE(\n",
    "                emb_table_sizes,\n",
    "                compr_config.dhe_m_exp,\n",
    "                compr_config.dhe_n_hash,\n",
    "                mlp_dims=dhe_mlp_dims,\n",
    "                mlp_activation=compr_config.dhe_mlp_activation,\n",
    "                batch_norm=compr_config.dhe_batch_norm,\n",
    "            )\n",
    "        else:\n",
    "            sys.exit(\n",
    "                \"ERROR: compression_type=\"\n",
    "                + compr_config.compression_type\n",
    "                + \" is not supported\"\n",
    "            )\n",
    "        return emb_l\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        compression_config: CompressionConfig,\n",
    "        emb_dim: int,\n",
    "        emb_table_sizes: list,\n",
    "        mlp_layers_size_bot: list,\n",
    "        mlp_layers_size_top: list,\n",
    "        arch_interaction_op: str,\n",
    "        arch_interaction_itself: bool = False,\n",
    "        sigmoid_bot: int = -1,\n",
    "        sigmoid_top: int = -1,\n",
    "        loss_threshold: float = 0.0,\n",
    "        loss_function: str = \"bce\",\n",
    "        loss_weights: str = None,\n",
    "    ) -> None:\n",
    "        self.compression_config = compression_config\n",
    "        super(CompressedDLRM, self).__init__(\n",
    "            emb_dim,\n",
    "            emb_table_sizes,\n",
    "            mlp_layers_size_bot,\n",
    "            mlp_layers_size_top,\n",
    "            arch_interaction_op,\n",
    "            arch_interaction_itself,\n",
    "            sigmoid_bot,\n",
    "            sigmoid_top,\n",
    "            loss_threshold,\n",
    "            loss_function,\n",
    "            loss_weights,\n",
    "        )\n",
    "\n",
    "    def apply_emb(self, lookup_ids: torch.Tensor) -> torch.Tensor:\n",
    "        return self.emb_list(lookup_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "It is convenient to introduce some dataclasses to collect model's hyperparameters and specify training/inference configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DLRMConfig:\n",
    "    # embedding size\n",
    "    arch_sparse_feature_size: int = 128\n",
    "    # bottom MLP layer output sizes\n",
    "    # last value needs to be arch_sparse_feature_size\n",
    "    arch_mlp_bot: str = \"512-256-128\"\n",
    "    # top MLP layer output sizes\n",
    "    # last value needs to be 1\n",
    "    arch_mlp_top: str = \"1024-1024-512-256-1\"\n",
    "    # interaction operator\n",
    "    arch_interaction_op: Literal[\"dot\", \"cat\"] = \"dot\"\n",
    "    # for \"dot\" interaction, whether to include the self-interactions e_i \\cdot e_i\n",
    "    arch_interaction_itself: bool = False\n",
    "    # if > 0, max number of rows per embedding table (Hashing Trick)\n",
    "    max_ind_range: int = -1\n",
    "    # embedding compression configuration\n",
    "    compression: CompressionConfig = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    # batch size PER DEVICE\n",
    "    mini_batch_size: int\n",
    "    # learning rate for MLP layers\n",
    "    learning_rate: float\n",
    "    # learning rate for embedding parameters\n",
    "    # (if None, defaults to learning_rate)\n",
    "    emb_learning_rate: float = None\n",
    "    # optimizer\n",
    "    # adam-sgd = Adam for MLP, SGD for embeddings\n",
    "    optimizer: Literal[\"sgd\", \"adam\", \"adam-sgd\"] = \"sgd\"\n",
    "    # SGD momentum\n",
    "    momentum: float = 0.0\n",
    "    # Adam betas\n",
    "    beta1: float = 0.9\n",
    "    beta2: float = 0.999\n",
    "    # learning rate scheduler\n",
    "    lr_scheduler: Literal[\"warmup-decay\", \"none\"] = \"none\"\n",
    "    # warmup-decay number of steps, as a fraction of 1 epoch\n",
    "    lr_num_warmup_steps: float = 0.05\n",
    "    lr_decay_start_step: float = 0.2\n",
    "    lr_num_decay_steps: float = 0.5\n",
    "    # lr_min * learning_rate is the minimum lr when decaying\n",
    "    lr_min: float = 0.001\n",
    "    # whether to schedule lr also for embedding parameters\n",
    "    schedule_emb_lr: bool = True\n",
    "    # loss function\n",
    "    loss_function: Literal[\"mse\", \"bce\", \"wbce\"] = \"bce\"\n",
    "    # loss weights for wbce loss\n",
    "    loss_weights: str = \"\"\n",
    "    # loss cropping threshold\n",
    "    loss_threshold: float = 0.0\n",
    "    # number of training epochs\n",
    "    nepochs: int = 1\n",
    "    # if > 0, stop training after a fixed number of steps\n",
    "    nsteps: int = 0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExecutionConfig:\n",
    "    # path to directory containing dataset\n",
    "    data_path: str\n",
    "    # #workers for dataloading\n",
    "    num_workers: int = 0\n",
    "    # shuffle batches\n",
    "    bin_shuffle: bool = False\n",
    "    # gradient accumulation factor\n",
    "    grad_accum: int = 1\n",
    "    # device iterations for training\n",
    "    ipu_device_iterations_training: int = 1\n",
    "    # device iterations for inference\n",
    "    ipu_device_iterations_inference: int = 1\n",
    "    # number of IPUs for training\n",
    "    ipu_replicas_training: int = 1\n",
    "    # number of IPUs for inference\n",
    "    ipu_replicas_inference: int = 1\n",
    "    # frequency of loss prints during training\n",
    "    print_freq: int = 100\n",
    "    # frequency of interleaved testing during training\n",
    "    test_freq: int = 10000\n",
    "    # batch size PER DEVICE for inference\n",
    "    test_mini_batch_size: int = 1024\n",
    "    # save model to path\n",
    "    save_model: str = \"\"\n",
    "    # load model from path\n",
    "    load_model: str = \"\"\n",
    "    # seed\n",
    "    seed: int = 12345\n",
    "    # Weights&Bias logging\n",
    "    wandb_logging: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once instantiated, the configuration classes can then be passed to the following function to build the (possibly compressed) DLRM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DLRM(\n",
    "    n_dense: int,\n",
    "    table_sizes: list,\n",
    "    dlrm_config: DLRMConfig,\n",
    "    exec_config: ExecutionConfig,\n",
    "    train_config: Optional[TrainingConfig] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Instantiate DLRM/CompressedDLRM model.\n",
    "\n",
    "    Args:\n",
    "        n_dense (int):\n",
    "            number of dense features\n",
    "        table_sizes (list):\n",
    "            number of categorical values for each sparse feature\n",
    "        dlrm_config (DLRMConfig):\n",
    "            DLRM configuration\n",
    "        exec_config (ExecutionConfig):\n",
    "            Execution configuration\n",
    "        train_config (Optional[TrainingConfig], optional):\n",
    "            Training configuration. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        the (possibly compressed) DLRM model\n",
    "    \"\"\"\n",
    "\n",
    "    arch_mlp_bot = np.fromstring(dlrm_config.arch_mlp_bot, sep=\"-\", dtype=int).tolist()\n",
    "    arch_mlp_top = np.fromstring(dlrm_config.arch_mlp_top, sep=\"-\", dtype=int).tolist()\n",
    "\n",
    "    emb_dim = dlrm_config.arch_sparse_feature_size\n",
    "    if arch_mlp_bot[-1] != emb_dim:\n",
    "        sys.exit(\n",
    "            \"ERROR: arch-sparse-feature-size \"\n",
    "            + str(emb_dim)\n",
    "            + \" does not match last dim of bottom mlp \"\n",
    "            + str(dlrm_config.arch_mlp_bot[-1])\n",
    "        )\n",
    "    if arch_mlp_top[-1] != 1:\n",
    "        sys.exit(\n",
    "            \"ERROR: last dim of top mlp \"\n",
    "            + str(dlrm_config.arch_mlp_top[-1])\n",
    "            + \" is not 1\"\n",
    "        )\n",
    "\n",
    "    # number of feature vectors to interact\n",
    "    num_fea = len(table_sizes) + 1\n",
    "    # compute number of interactions\n",
    "    if dlrm_config.arch_interaction_op == \"dot\":\n",
    "        if dlrm_config.arch_interaction_itself:\n",
    "            num_int = (num_fea * (num_fea + 1)) // 2 + emb_dim\n",
    "        else:\n",
    "            num_int = (num_fea * (num_fea - 1)) // 2 + emb_dim\n",
    "    elif dlrm_config.arch_interaction_op == \"cat\":\n",
    "        num_int = num_fea * emb_dim\n",
    "    else:\n",
    "        sys.exit(\n",
    "            \"ERROR: arch-interaction-op=\"\n",
    "            + dlrm_config.arch_interaction_op\n",
    "            + \" is not supported\"\n",
    "        )\n",
    "    # complete mlp layer sizes with input dimensions\n",
    "    arch_mlp_bot_adjusted = [n_dense] + arch_mlp_bot\n",
    "    arch_mlp_top_adjusted = [num_int] + arch_mlp_top\n",
    "\n",
    "    # Hashing Trick\n",
    "    if dlrm_config.max_ind_range > 0:\n",
    "        table_sizes = [min(t_size, dlrm_config.max_ind_range) for t_size in table_sizes]\n",
    "\n",
    "    init_options = dict(\n",
    "        emb_dim=dlrm_config.arch_sparse_feature_size,\n",
    "        emb_table_sizes=table_sizes,\n",
    "        mlp_layers_size_bot=arch_mlp_bot_adjusted,\n",
    "        mlp_layers_size_top=arch_mlp_top_adjusted,\n",
    "        arch_interaction_op=dlrm_config.arch_interaction_op,\n",
    "        arch_interaction_itself=dlrm_config.arch_interaction_itself,\n",
    "        sigmoid_bot=-1,\n",
    "        sigmoid_top=(\n",
    "            len(arch_mlp_top_adjusted) - 2\n",
    "        ),  # sigmoid for last top MLP layer to obtain CTR probabilities\n",
    "    )\n",
    "    if train_config:\n",
    "        init_options.update(\n",
    "            dict(\n",
    "                loss_threshold=train_config.loss_threshold,\n",
    "                loss_function=train_config.loss_function,\n",
    "                loss_weights=train_config.loss_weights,\n",
    "            )\n",
    "        )\n",
    "    if dlrm_config.compression:\n",
    "        init_options.update(dict(compression_config=dlrm_config.compression))\n",
    "        model = CompressedDLRM(**init_options)\n",
    "    else:\n",
    "        model = DLRM(**init_options)\n",
    "\n",
    "    # Load checkpoint\n",
    "    if not exec_config.load_model == \"\":\n",
    "        print(\"Loading saved model {}\".format(exec_config.load_model))\n",
    "        ld_model = torch.load(exec_config.load_model, map_location=torch.device(\"cpu\"))\n",
    "        model.load_state_dict(ld_model[\"state_dict\"])\n",
    "\n",
    "        print(\n",
    "            \"Saved at: epoch = {:d}/{:d}, step = {:d}/{:d}\".format(\n",
    "                ld_model[\"epoch\"] + 1,\n",
    "                ld_model[\"nepochs\"],\n",
    "                ld_model[\"step\"],\n",
    "                ld_model[\"nbatches\"],\n",
    "            )\n",
    "        )\n",
    "        print(\"Training state: loss = {:.5f}\".format(ld_model[\"train_loss\"]))\n",
    "        print(\"Testing state: AUC = {:.5f}\".format(ld_model[\"test_auc\"]))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline\n",
    "\n",
    "The following code is based on the implementation in the DLRM [repository](https://github.com/facebookresearch/dlrm/blob/main/dlrm_data_pytorch.py) and is used to load the preprocessed *Criteo 1TB* dataset and build the PyTorch dataloaders that we will use for training/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_criteo_data_and_loaders(\n",
    "    exec_config: ExecutionConfig,\n",
    "    train_config: Optional[TrainingConfig] = None,\n",
    "    max_ind_range: int = -1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate PyTorch datasets and dataloaders for Criteo 1TB.\n",
    "\n",
    "    Args:\n",
    "        exec_config (ExecutionConfig):\n",
    "            Execution configuration.\n",
    "        train_config (Optional[TrainingConfig], optional):\n",
    "            Training configuration; if not provided, only the test\n",
    "            datasets/dataloader are returned. Defaults to None.\n",
    "        max_ind_range (int, optional):\n",
    "            If > 0, limit the number of rows of embedding tables\n",
    "            to this value. Defaults to -1.\n",
    "\n",
    "    Returns:\n",
    "        Test (and possibly training) dataset and dataloader.\n",
    "    \"\"\"\n",
    "    # path of pre-processed Criteo 1TB source files\n",
    "    d_path = exec_config.data_path\n",
    "    test_file = os.path.join(d_path, \"terabyte_processed_test.bin\")\n",
    "    counts_file = os.path.join(d_path, \"day_fea_count.npz\")\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    if train_config:\n",
    "        train_file = os.path.join(d_path, \"terabyte_processed_train.bin\")\n",
    "        train_data = CriteoBinDataset(\n",
    "            data_file=train_file,\n",
    "            counts_file=counts_file,\n",
    "            batch_size=train_config.mini_batch_size\n",
    "            * exec_config.grad_accum\n",
    "            * exec_config.ipu_device_iterations_training\n",
    "            * exec_config.ipu_replicas_training,\n",
    "            max_ind_range=max_ind_range,\n",
    "        )\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_data,\n",
    "            batch_size=None,\n",
    "            batch_sampler=None,\n",
    "            shuffle=False,\n",
    "            num_workers=exec_config.num_workers,\n",
    "            pin_memory=False,\n",
    "            drop_last=False,\n",
    "            sampler=torch.utils.data.RandomSampler(train_data)\n",
    "            if exec_config.bin_shuffle\n",
    "            else None,\n",
    "        )\n",
    "        out[\"train_data\"] = train_data\n",
    "        out[\"train_loader\"] = train_loader\n",
    "\n",
    "    test_data = CriteoBinDataset(\n",
    "        data_file=test_file,\n",
    "        counts_file=counts_file,\n",
    "        batch_size=exec_config.test_mini_batch_size\n",
    "        * exec_config.ipu_device_iterations_inference\n",
    "        * exec_config.ipu_replicas_inference,\n",
    "        max_ind_range=max_ind_range,\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=None,\n",
    "        batch_sampler=None,\n",
    "        shuffle=False,\n",
    "        num_workers=exec_config.num_workers,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    out[\"test_data\"] = test_data\n",
    "    out[\"test_loader\"] = test_loader\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the compressed model\n",
    "\n",
    "Due to the size of embedding tables, running the *uncompressed* DLRM model on AI accelerators requires **model parallelism**, which is typically implemented by partitioning tables across devices (see for instance the `parallel_forward` method in Meta's [implementation](https://github.com/facebookresearch/dlrm/blob/main/dlrm_s_pytorch.py#L605)). This introduces large communication costs for both training and inference, due to the need of communicating embeddings from the device where they are stored to the one where they need to be used for batch computations (and viceversa for gradients). As a consequence, while increasing the number of devices enables the use of larger embedding tables, it does not bring much speed gain: embedding lookup, when implemented this way, can take up to 50/60% of the total runtime [[Mudigere *et al.*, 2022](https://dl.acm.org/doi/abs/10.1145/3470496.3533727)], introducing significant scaling bottlenecks.\n",
    "\n",
    "Communication costs can be greatly reduced when using compression techniques for embeddings, especially in the case where the compressed model fits in the memory of a single device. Indeed, the extra computation time required to generate embeddings on-the-fly is expected to be much smaller than the all-to-all communication costs that we would otherwise need to look up embeddings [[Desai *et al.*, 2022](https://proceedings.neurips.cc/paper_files/paper/2022/hash/dbae915128892556134f1c5375855590-Abstract-Conference.html)]. Distributing workload across multiple accelerators can then be done in an **entirely data-parallel way**. This is the paradigm that we will follow in our implementation, taking advantage of the IPU's large and fast-access on-chip memory to store all parameters of the compressed models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester:\n",
    "    \"\"\"\n",
    "    Utility class to run inference on test dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Union[DLRM, CompressedDLRM],\n",
    "        test_dl,\n",
    "        exec_config: ExecutionConfig,\n",
    "        logger: Optional[WandBLogger] = None,\n",
    "    ):\n",
    "        self.exec_config = exec_config\n",
    "        self.test_dl = test_dl\n",
    "        if (\n",
    "            isinstance(model, CompressedDLRM)\n",
    "            and model.compression_config.compression_type == \"comp_emb\"\n",
    "        ):\n",
    "            self.comp_emb = True\n",
    "            self.comp_emb_P, _, _, self.comp_emb_A = model.emb_list.random_numbers\n",
    "        else:\n",
    "            self.comp_emb = False\n",
    "\n",
    "        self.logger = logger\n",
    "\n",
    "        # poptorch options\n",
    "        inf_options = poptorch.Options()\n",
    "        inf_options.deviceIterations(exec_config.ipu_device_iterations_inference)\n",
    "        inf_options.replicationFactor(exec_config.ipu_replicas_inference)\n",
    "\n",
    "        self.poptorch_inference = poptorch.inferenceModel(model, options=inf_options)\n",
    "\n",
    "        print(\"Compiling inference model...\")\n",
    "        st_compile = time.time()\n",
    "        _ = self.poptorch_inference(*next(iter(test_dl))[:-1])\n",
    "        if self.logger:\n",
    "            self.logger.log(\n",
    "                \"compilation_inf\",\n",
    "                {\"time\": {\"inference_model_compile_time\": time.time() - st_compile}},\n",
    "            )\n",
    "\n",
    "    def test(self, best_auc_test: float = 0.0):\n",
    "        n_test_sample = (\n",
    "            len(self.test_dl)\n",
    "            * self.exec_config.test_mini_batch_size\n",
    "            * self.exec_config.ipu_replicas_inference\n",
    "            * self.exec_config.ipu_device_iterations_inference\n",
    "        )\n",
    "        print(\n",
    "            f\"Testing on {n_test_sample} elements \"\n",
    "            f\"using {self.exec_config.ipu_replicas_inference} IPUs\"\n",
    "        )\n",
    "        scores = []\n",
    "        targets = []\n",
    "\n",
    "        # inference on IPU\n",
    "        start_testing = time.time()\n",
    "        for testBatch in iter(self.test_dl):\n",
    "            X_test, lS_i_test, T_test = testBatch\n",
    "            if self.comp_emb:\n",
    "                # for Compositional Embedding compression, pre-compute A*lS_i on host (int64)\n",
    "                # and stream remainder mod P to device (int32)\n",
    "                lS_i_test = (lS_i_test * self.comp_emb_A) % self.comp_emb_P\n",
    "\n",
    "            # forward pass\n",
    "            Z_test = self.poptorch_inference(\n",
    "                X_test,\n",
    "                lS_i_test,\n",
    "            )\n",
    "            scores.append(Z_test)\n",
    "            targets.append(T_test)\n",
    "\n",
    "        # compute AUROC score on CPU\n",
    "        start_metrics = time.time()\n",
    "        scores = torch.concat(scores, dim=0)\n",
    "        targets = torch.concat(targets, dim=0)\n",
    "        auc_test = roc_auc_score(targets, scores)\n",
    "        if isinstance(auc_test, list):\n",
    "            auc_test = auc_test[0]\n",
    "\n",
    "        is_best = auc_test > best_auc_test\n",
    "        if is_best:\n",
    "            best_auc_test = auc_test\n",
    "\n",
    "        val_time_total = time.time() - start_testing\n",
    "        val_time_on_ipu = start_metrics - start_testing\n",
    "        print(\n",
    "            \" AUC {:.4f}, best AUC {:.4f},\".format(auc_test, best_auc_test)\n",
    "            + \" validation_time (on IPU) {:3.3f} s,\".format(val_time_on_ipu)\n",
    "            + \" validation_time (total) {:3.3f} s\".format(val_time_total),\n",
    "            flush=True,\n",
    "        )\n",
    "\n",
    "        if self.logger:\n",
    "            self.logger.log(\n",
    "                \"val\",\n",
    "                {\n",
    "                    \"validation\": {\n",
    "                        \"val_auc\": auc_test,\n",
    "                        \"best_val_auc\": best_auc_test,\n",
    "                    },\n",
    "                    \"time\": {\n",
    "                        \"val_time_on_ipu\": val_time_on_ipu,\n",
    "                        \"val_time_total\": val_time_total,\n",
    "                    },\n",
    "                },\n",
    "            )\n",
    "\n",
    "        return auc_test, is_best\n",
    "\n",
    "    def destroy(self):\n",
    "        # detach poptorch runner from IPUs\n",
    "        self.poptorch_inference.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Compressing embeddings also has interesting consequences on the training regime. The uncompressed DLRM is typically trained with basic SGD [[Naumov *et al.*, 2019](https://arxiv.org/abs/1906.00091)], as more advanced optimizers like Adam or LAMB require storing the first- and second-order moments for every weight, further increasing the already-challenging memory footprint of the model by a prohibitive factor of 3x. Moreover, in order to achieve efficient implementations, sparse optimizers should be preferred, as only a small fraction of all embeddings are used when processing a batch and therefore need to be updated. However, not all machine learning frameworks support sparse gradients in a seamless way, thus introducing additional engineering challenges. The reduced parameter counts of compressed models remove constraints on the choice of the optimizer, and — by looking at how embeddings are generated in the Compositional Embedding and DHE modules — there is no longer a reason why dense gradients should be avoided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRPolicyScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
    "    \"\"\"\n",
    "    Warmup-decay learning rate scheduler.\n",
    "    Derived from https://github.com/facebookresearch/dlrm/blob/main/dlrm_s_pytorch.py#L157\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer,\n",
    "        num_warmup_steps,\n",
    "        decay_start_step,\n",
    "        num_decay_steps,\n",
    "        min_lr,\n",
    "    ):\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        self.decay_start_step = decay_start_step\n",
    "        self.decay_end_step = self.decay_start_step + num_decay_steps\n",
    "        self.num_decay_steps = num_decay_steps\n",
    "        self.min_lr = min_lr\n",
    "\n",
    "        if self.decay_start_step < self.num_warmup_steps:\n",
    "            sys.exit(\"Learning rate warmup must finish before the decay starts\")\n",
    "\n",
    "        super(LRPolicyScheduler, self).__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_count = self._step_count\n",
    "        if step_count < self.num_warmup_steps:\n",
    "            # warmup\n",
    "            scale = 1.0 - (self.num_warmup_steps - step_count) / self.num_warmup_steps\n",
    "            lr = [base_lr * scale for base_lr in self.base_lrs]\n",
    "            self.last_lr = lr\n",
    "        elif self.decay_start_step <= step_count and step_count < self.decay_end_step:\n",
    "            # decay\n",
    "            decayed_steps = step_count - self.decay_start_step\n",
    "            scale = ((self.num_decay_steps - decayed_steps) / self.num_decay_steps) ** 2\n",
    "            lr = [max(self.min_lr, base_lr * scale) for base_lr in self.base_lrs]\n",
    "            self.last_lr = lr\n",
    "        else:\n",
    "            if self.num_decay_steps > 0:\n",
    "                # freeze at last, either because we're after decay\n",
    "                # or because we're between warmup and decay\n",
    "                lr = self.last_lr\n",
    "            else:\n",
    "                # do not adjust\n",
    "                lr = self.base_lrs\n",
    "        return lr\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Utility class to run training with interleaved validation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Union[DLRM, CompressedDLRM],\n",
    "        train_dl,\n",
    "        test_dl,\n",
    "        train_config: TrainingConfig,\n",
    "        exec_config: ExecutionConfig,\n",
    "        logger: Optional[WandBLogger] = None,\n",
    "    ) -> None:\n",
    "        self.train_config = train_config\n",
    "        self.exec_config = exec_config\n",
    "        if (\n",
    "            isinstance(model, CompressedDLRM)\n",
    "            and model.compression_config.compression_type == \"comp_emb\"\n",
    "        ):\n",
    "            self.comp_emb = True\n",
    "            self.comp_emb_P, _, _, self.comp_emb_A = model.emb_list.random_numbers\n",
    "        else:\n",
    "            self.comp_emb = False\n",
    "\n",
    "        # optimizer and LR scheduler\n",
    "        self.optimizer = self.get_optimizer(model, self.train_config, self.exec_config)\n",
    "        self.lr_scheduler = self.get_lr_scheduler(\n",
    "            self.optimizer, self.train_config, self.exec_config, len(train_dl)\n",
    "        )\n",
    "        self.current_lr_mlp = self.optimizer.param_groups[0][\"lr\"]\n",
    "        self.current_lr_emb = self.optimizer.param_groups[-1][\"lr\"]\n",
    "        if self.train_config.optimizer == \"adam-sgd\":\n",
    "            self.current_lr_emb /= self.big_eps\n",
    "\n",
    "        self.train_dl = train_dl\n",
    "\n",
    "        self.logger = logger\n",
    "\n",
    "        # Tester for interleaved validation\n",
    "        self.tester = Tester(model, test_dl, exec_config, self.logger)\n",
    "        if (\n",
    "            self.exec_config.ipu_replicas_training\n",
    "            + self.exec_config.ipu_replicas_inference\n",
    "            > NUM_AVAILABLE_IPUS\n",
    "        ):\n",
    "            self.tester.poptorch_inference.detachFromDevice()\n",
    "\n",
    "        # poptorch training options\n",
    "        tr_options = poptorch.Options()\n",
    "        tr_options.Training.gradientAccumulation(exec_config.grad_accum)\n",
    "        tr_options.deviceIterations(exec_config.ipu_device_iterations_training)\n",
    "        tr_options.replicationFactor(exec_config.ipu_replicas_training)\n",
    "        tr_options.Training.setMeanAccumulationAndReplicationReductionStrategy(\n",
    "            poptorch.MeanReductionStrategy.Running\n",
    "        )\n",
    "        tr_options.outputMode(poptorch.OutputMode.Sum)\n",
    "        tr_options._Popart.set(\"saveInitializersToFile\", \"dlrm_weight.onnx\")\n",
    "\n",
    "        self.poptorch_trainer = poptorch.trainingModel(\n",
    "            model, options=tr_options, optimizer=self.optimizer\n",
    "        )\n",
    "\n",
    "        print(\"Compiling training model...\")\n",
    "        st_compile = time.time()\n",
    "        _ = self.poptorch_trainer(*next(iter(train_dl)))\n",
    "        if self.logger:\n",
    "            self.logger.log(\n",
    "                \"compilation_tr\",\n",
    "                {\"time\": {\"train_model_compile_time\": time.time() - st_compile}},\n",
    "            )\n",
    "\n",
    "    def get_optimizer(\n",
    "        self,\n",
    "        model: Union[DLRM, CompressedDLRM],\n",
    "        config: TrainingConfig,\n",
    "        exec_config: ExecutionConfig,\n",
    "    ):\n",
    "        # learning rate for embedding parameters\n",
    "        self.emb_lr = (\n",
    "            config.emb_learning_rate\n",
    "            if config.emb_learning_rate\n",
    "            else config.learning_rate\n",
    "        )\n",
    "        if config.optimizer == \"sgd\":\n",
    "            optimizer = poptorch.optim.SGD(\n",
    "                [\n",
    "                    {\n",
    "                        \"params\": model.bot_mlp.parameters(),\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": model.top_mlp.parameters(),\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": model.emb_list.parameters(),\n",
    "                        \"lr\": self.emb_lr,\n",
    "                    },\n",
    "                ],\n",
    "                lr=config.learning_rate,\n",
    "                momentum=config.momentum if config.momentum > 0 else None,\n",
    "            )\n",
    "        elif config.optimizer == \"adam-sgd\":\n",
    "            # as poptorch does not support using different optimizers for different\n",
    "            # parameter groups, we simulate SGD with Adam for embedding parameters\n",
    "            # by setting beta1=0, eps >> 0, and scaling learning rate by eps\n",
    "            self.big_eps = 1e7\n",
    "            self.emb_lr *= self.big_eps\n",
    "            optimizer = poptorch.optim.Adam(\n",
    "                [\n",
    "                    {\n",
    "                        \"params\": model.bot_mlp.parameters(),\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": model.top_mlp.parameters(),\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": model.emb_list.parameters(),\n",
    "                        \"lr\": self.emb_lr,\n",
    "                        \"betas\": (0.0, config.beta2),\n",
    "                        \"eps\": self.big_eps,\n",
    "                    },\n",
    "                ],\n",
    "                lr=config.learning_rate,\n",
    "                betas=(config.beta1, config.beta2),\n",
    "                eps=1e-8,\n",
    "            )\n",
    "        elif config.optimizer == \"adam\":\n",
    "            optimizer = poptorch.optim.Adam(\n",
    "                [\n",
    "                    {\n",
    "                        \"params\": model.bot_mlp.parameters(),\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": model.top_mlp.parameters(),\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": model.emb_list.parameters(),\n",
    "                        \"lr\": self.emb_lr,\n",
    "                    },\n",
    "                ],\n",
    "                lr=config.learning_rate,\n",
    "                betas=(config.beta1, config.beta2),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {config.optimizer}\")\n",
    "\n",
    "        if not exec_config.load_model == \"\":\n",
    "            # load checkpoint\n",
    "            ld_model = torch.load(\n",
    "                exec_config.load_model, map_location=torch.device(\"cpu\")\n",
    "            )\n",
    "            optimizer.load_state_dict(ld_model[\"opt_state_dict\"])\n",
    "            self.best_auc_test = ld_model[\"test_auc\"]\n",
    "            self.total_samples = ld_model[\"total_samples\"]\n",
    "            self.skip_upto_epoch = ld_model[\"epoch\"]\n",
    "            self.skip_upto_step = ld_model[\"step\"]\n",
    "        else:\n",
    "            self.best_auc_test = 0.0\n",
    "            self.total_samples = 0\n",
    "            self.skip_upto_epoch = 0\n",
    "            self.skip_upto_step = 0\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def get_lr_scheduler(\n",
    "        self,\n",
    "        optimizer,\n",
    "        config: TrainingConfig,\n",
    "        exec_config: ExecutionConfig,\n",
    "        steps_per_epoch: int,\n",
    "    ):\n",
    "        if config.lr_scheduler == \"warmup-decay\":\n",
    "            lr_scheduler = LRPolicyScheduler(\n",
    "                optimizer,\n",
    "                int(config.lr_num_warmup_steps * steps_per_epoch),\n",
    "                int(config.lr_decay_start_step * steps_per_epoch),\n",
    "                int(config.lr_num_decay_steps * steps_per_epoch),\n",
    "                config.lr_min * config.learning_rate,\n",
    "            )\n",
    "\n",
    "            if not exec_config.load_model == \"\":\n",
    "                # load checkpoint\n",
    "                ld_model = torch.load(\n",
    "                    exec_config.load_model, map_location=torch.device(\"cpu\")\n",
    "                )\n",
    "                if ld_model[\"lr_scheduler_state_dict\"]:\n",
    "                    lr_scheduler.load_state_dict(ld_model[\"lr_scheduler_state_dict\"])\n",
    "                for i, param_group in enumerate(optimizer.param_groups):\n",
    "                    param_group[\"lr\"] = ld_model[\"opt_state_dict\"][\"param_groups\"][i][\n",
    "                        \"lr\"\n",
    "                    ]\n",
    "            optimizer._step_count = lr_scheduler._step_count\n",
    "        else:\n",
    "            lr_scheduler = None\n",
    "            optimizer._step_count = 1\n",
    "\n",
    "        return lr_scheduler\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        # dense_features, sparse_features_ids, target\n",
    "        X, lS_i, T = batch\n",
    "        # for Compositional Embedding compression, pre-compute A*lS_i on host (int64)\n",
    "        # and stream remainder mod P to device (int32)\n",
    "        if self.comp_emb:\n",
    "            lS_i = (lS_i * self.comp_emb_A) % self.comp_emb_P\n",
    "\n",
    "        _, loss = self.poptorch_trainer(X, lS_i, T)\n",
    "        if torch.any(torch.isnan(loss)):\n",
    "            sys.exit(\"NaNs in loss\")\n",
    "        # loss is returned per replica, averaged over the microbatches\n",
    "        loss = loss.sum() * self.train_config.mini_batch_size\n",
    "\n",
    "        if self.lr_scheduler:\n",
    "            self.lr_scheduler.step()\n",
    "            self.current_lr_mlp = self.optimizer.param_groups[0][\"lr\"]\n",
    "            if not self.train_config.schedule_emb_lr:\n",
    "                self.optimizer.param_groups[-1][\"lr\"] = self.emb_lr\n",
    "            self.current_lr_emb = self.optimizer.param_groups[-1][\"lr\"]\n",
    "            if self.train_config.optimizer == \"adam-sgd\":\n",
    "                self.current_lr_emb /= self.big_eps\n",
    "            self.poptorch_trainer.setOptimizer(self.optimizer)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Adapted from https://github.com/facebookresearch/dlrm/blob/main/dlrm_s_pytorch.py\n",
    "        \"\"\"\n",
    "        total_loss = 0.0\n",
    "        iters = 0\n",
    "        samples = 0\n",
    "\n",
    "        nsteps = (\n",
    "            self.train_config.nsteps\n",
    "            if self.train_config.nsteps > 0\n",
    "            else len(self.train_dl)\n",
    "        )\n",
    "\n",
    "        prev_time = time.time()\n",
    "        stop_training = False\n",
    "        for k in range(self.train_config.nepochs):\n",
    "            if stop_training:\n",
    "                break\n",
    "            if k < self.skip_upto_epoch:\n",
    "                continue\n",
    "            for j, inputBatch in enumerate(self.train_dl):\n",
    "                # early exit if nsteps was set by the user and has been exceeded\n",
    "                if j >= nsteps:\n",
    "                    stop_training = True\n",
    "                    break\n",
    "                if k == self.skip_upto_epoch and j < self.skip_upto_step:\n",
    "                    continue\n",
    "\n",
    "                # forward and backward step\n",
    "                loss = self.train_step(inputBatch)\n",
    "                total_loss += loss\n",
    "\n",
    "                iters += 1\n",
    "                n_batch_samples = inputBatch[-1].numel()\n",
    "                samples += n_batch_samples\n",
    "                self.total_samples += n_batch_samples\n",
    "\n",
    "                should_print = ((j + 1) % self.exec_config.print_freq == 0) or (\n",
    "                    j + 1 == nsteps\n",
    "                )\n",
    "                should_test = (self.exec_config.test_freq > 0) and (\n",
    "                    ((j + 1) % self.exec_config.test_freq == 0) or (j + 1 == nsteps)\n",
    "                )\n",
    "\n",
    "                # print time, loss and metrics\n",
    "                if should_print or should_test:\n",
    "                    new_time = time.time()\n",
    "                    gT = 1000.0 * (new_time - prev_time) / iters\n",
    "                    iters = 0\n",
    "\n",
    "                    # average loss from last print\n",
    "                    train_loss = total_loss / samples\n",
    "                    total_loss = 0.0\n",
    "                    samples = 0\n",
    "\n",
    "                    print(\n",
    "                        \"Finished training step {}/{} of epoch {}, {:.2f} ms/step,\".format(\n",
    "                            j + 1,\n",
    "                            len(self.train_dl),\n",
    "                            k,\n",
    "                            gT,\n",
    "                        )\n",
    "                        + \" loss {:.6f}\".format(train_loss)\n",
    "                        + \" ({})\".format(time.strftime(\"%H:%M:%S\")),\n",
    "                        flush=True,\n",
    "                    )\n",
    "\n",
    "                    if self.logger:\n",
    "                        self.logger.log(\n",
    "                            \"train_step\",\n",
    "                            {\n",
    "                                \"train\": {\n",
    "                                    \"loss\": train_loss,\n",
    "                                    \"lr\": self.current_lr_mlp,\n",
    "                                    \"lr_emb\": self.current_lr_emb,\n",
    "                                    \"samples\": self.total_samples,\n",
    "                                },\n",
    "                                \"time\": {\n",
    "                                    \"avg_train_iteration_time\": gT / 1000,\n",
    "                                    \"train_loop_step_time\": new_time - prev_time,\n",
    "                                },\n",
    "                            },\n",
    "                        )\n",
    "\n",
    "                    prev_time = new_time\n",
    "\n",
    "                # interleaved validation\n",
    "                if should_test:\n",
    "                    if (\n",
    "                        self.exec_config.ipu_replicas_training\n",
    "                        + self.exec_config.ipu_replicas_inference\n",
    "                        <= NUM_AVAILABLE_IPUS\n",
    "                    ):\n",
    "                        self.poptorch_trainer.copyWeightsToHost()\n",
    "                        self.tester.poptorch_inference.copyWeightsToDevice()\n",
    "                    else:\n",
    "                        self.poptorch_trainer.detachFromDevice()\n",
    "                        self.tester.poptorch_inference.attachToDevice()\n",
    "\n",
    "                    print(\n",
    "                        \"Testing at step {}/{} of epoch {}\".format(\n",
    "                            j + 1, len(self.train_dl), k\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    auc_test, is_best = self.tester.test(\n",
    "                        self.best_auc_test,\n",
    "                    )\n",
    "\n",
    "                    # if test AUROC improved, save checkpoint\n",
    "                    if is_best:\n",
    "                        self.best_auc_test = auc_test\n",
    "                        if not (self.exec_config.save_model == \"\"):\n",
    "                            model_metrics_dict = {\n",
    "                                \"nepochs\": self.train_config.nepochs,\n",
    "                                \"nbatches\": len(self.train_dl),\n",
    "                                \"epoch\": k,\n",
    "                                \"step\": j + 1,\n",
    "                                \"total_samples\": self.total_samples,\n",
    "                                \"train_loss\": train_loss,\n",
    "                                \"state_dict\": self.poptorch_trainer.state_dict(),\n",
    "                                \"opt_state_dict\": self.optimizer.state_dict(),\n",
    "                                \"lr_scheduler_state_dict\": self.lr_scheduler.state_dict()\n",
    "                                if self.lr_scheduler\n",
    "                                else None,\n",
    "                                \"test_auc\": auc_test,\n",
    "                            }\n",
    "                            print(\n",
    "                                \"Saving model to {}\".format(self.exec_config.save_model)\n",
    "                            )\n",
    "                            torch.save(model_metrics_dict, self.exec_config.save_model)\n",
    "\n",
    "                    if (\n",
    "                        self.exec_config.ipu_replicas_training\n",
    "                        + self.exec_config.ipu_replicas_inference\n",
    "                        > NUM_AVAILABLE_IPUS\n",
    "                    ):\n",
    "                        self.tester.poptorch_inference.detachFromDevice()\n",
    "                        self.poptorch_trainer.attachToDevice()\n",
    "\n",
    "                    prev_time = time.time()\n",
    "\n",
    "    def destroy(self):\n",
    "        # detach poptorch runners from IPUs\n",
    "        self.poptorch_trainer.destroy()\n",
    "        self.tester.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "The [MLPerf recommendation benchmark](https://mlcommons.org/en/training-normal-21/) on *Criteo 1TB* sets as quality target for the DLRM model reaching an AUC score of 0.8025. The [reference implementation](https://github.com/facebookresearch/dlrm/blob/main/bench/run_and_time.sh) uses embedding size `128` and sets the maximum number of rows for each table to `M = 4e7`, resulting in ~100GB of memory required to store embeddings (we computed this at the beginning of section [*Embedding compression*](#embedding-compression)). Such a model is expected to reach the quality target approximately after one epoch of training (see for instance [[Desai *et al.*, 2022](https://proceedings.neurips.cc/paper_files/paper/2022/hash/dbae915128892556134f1c5375855590-Abstract-Conference.html)]).\n",
    "\n",
    "Our objective is to show that **the quality target can still be reached by a compressed DLRM even after reducing the parameter count for embeddings by a factor of thousands**. We will test the three different compression techniques introduced in [*Embedding compression*](#embedding-compression), namely Hashing Trick, Compositional Embedding and DHE, with a similar number of trainable parameters for fair comparison. All other settings in the DLRM configuration (embedding size, interaction type, layer sizes of bottom and top MLPs, etc) will be shared across models and taken from the MLPerf reference implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference implementation architecture\n",
    "embedding_size = 128\n",
    "bottom_mlp_arch = \"512-256-128\"\n",
    "top_mlp_arch = \"1024-1024-512-256-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    \"\"\"\n",
    "    Utility class to setup experiments and run training/inference\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dlrm_config: DLRMConfig,\n",
    "        exec_config: ExecutionConfig,\n",
    "        train_config: Optional[TrainingConfig] = None,\n",
    "    ):\n",
    "        # reproducibility\n",
    "        np.random.seed(exec_config.seed)\n",
    "        torch.manual_seed(exec_config.seed)\n",
    "\n",
    "        self.dls = make_criteo_data_and_loaders(\n",
    "            exec_config, train_config, dlrm_config.max_ind_range\n",
    "        )\n",
    "\n",
    "        with np.load(\n",
    "            Path(exec_config.data_path).joinpath(\"day_fea_count.npz\")\n",
    "        ) as counts:\n",
    "            table_sizes = counts[\"counts\"].tolist()\n",
    "        self.model = create_DLRM(\n",
    "            self.dls[\"test_data\"].den_fea,\n",
    "            table_sizes,\n",
    "            dlrm_config,\n",
    "            exec_config,\n",
    "            train_config,\n",
    "        )\n",
    "\n",
    "        if exec_config.wandb_logging:\n",
    "            self.logger = WandBLogger(\n",
    "                self.model, dlrm_config, exec_config, train_config\n",
    "            )\n",
    "        else:\n",
    "            self.logger = None\n",
    "\n",
    "        if train_config:\n",
    "            self.trainer = Trainer(\n",
    "                self.model,\n",
    "                self.dls[\"train_loader\"],\n",
    "                self.dls[\"test_loader\"],\n",
    "                train_config,\n",
    "                exec_config,\n",
    "                self.logger,\n",
    "            )\n",
    "            self.tester = self.trainer.tester\n",
    "        else:\n",
    "            self.trainer = None\n",
    "            self.tester = Tester(\n",
    "                self.model,\n",
    "                self.dls[\"test_loader\"],\n",
    "                exec_config,\n",
    "                self.logger,\n",
    "            )\n",
    "\n",
    "    def run_training(self) -> None:\n",
    "        if not self.trainer:\n",
    "            raise ValueError(\"No training configuration specified\")\n",
    "        if not self.trainer.poptorch_trainer.isAttachedToDevice():\n",
    "            if self.tester.poptorch_inference.isAttachedToDevice():\n",
    "                self.tester.poptorch_inference.detachFromDevice()\n",
    "            self.trainer.poptorch_trainer.attachToDevice()\n",
    "\n",
    "        self.trainer.train()\n",
    "\n",
    "    def run_inference(self) -> None:\n",
    "        if not self.tester.poptorch_inference.isAttachedToDevice():\n",
    "            if self.trainer and self.trainer.poptorch_trainer.isAttachedToDevice():\n",
    "                self.trainer.poptorch_trainer.detachFromDevice()\n",
    "            self.tester.poptorch_inference.attachToDevice()\n",
    "\n",
    "        self.tester.test()\n",
    "\n",
    "    def destroy(self):\n",
    "        if self.trainer:\n",
    "            self.trainer.destroy()\n",
    "        else:\n",
    "            self.tester.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example on how to use the `Experiment` class, let us run a few training steps of the DLRM model with embedding compression via Hashing Trick. When instantiating the `Experiment`, the inference model (and possibly the training one, if a training configuration has been provided) are compiled. As specified in the `TrainingConfig` and `ExecutionConfig`, this model will run on 4 IPUs (in a purely data-parallel way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data file: /net/group/research/criteo_datasets/criteo_terabyte/terabyte_processed_train.bin number of batches: 1024218\n",
      "data file: /net/group/research/criteo_datasets/criteo_terabyte/terabyte_processed_test.bin number of batches: 21762\n",
      "Compiling inference model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:23:29.508] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 44\n",
      "Graph compilation: 100%|██████████| 100/100 [00:47<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:24:21.465] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 216\n",
      "Graph compilation: 100%|██████████| 100/100 [01:46<00:00]\n"
     ]
    }
   ],
   "source": [
    "# Hashing Trick\n",
    "max_ind_range = 5000\n",
    "\n",
    "ht_dlrm_config = DLRMConfig(\n",
    "    embedding_size,\n",
    "    bottom_mlp_arch,\n",
    "    top_mlp_arch,\n",
    "    max_ind_range=max_ind_range,\n",
    ")\n",
    "ht_exec_config = exec_config = ExecutionConfig(\n",
    "    data_path=dataset_dir,\n",
    "    ipu_replicas_training=4,\n",
    "    ipu_replicas_inference=NUM_AVAILABLE_IPUS,\n",
    "    ipu_device_iterations_training=2,\n",
    "    print_freq=1024,\n",
    "    test_freq=204800,\n",
    "    # wandb_logging=True,\n",
    ")\n",
    "ht_train_config = TrainingConfig(\n",
    "    mini_batch_size=512,\n",
    "    optimizer=\"sgd\",\n",
    "    learning_rate=1.0,\n",
    "    nsteps=10000,  # limit run to 10000 train steps\n",
    ")\n",
    "\n",
    "ht_experiment = Experiment(\n",
    "    dlrm_config=ht_dlrm_config,\n",
    "    exec_config=ht_exec_config,\n",
    "    train_config=ht_train_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then just use `Experiment.run_training` to train for the specified number of steps. This will also perform interleaved validation on the test set every `ExecutionConfig.test_freq` steps, and a final testing at the end of the run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training step 1024/1024218 of epoch 0, 12.36 ms/step, loss 0.135162 (16:41:27)\n",
      "Finished training step 2048/1024218 of epoch 0, 12.96 ms/step, loss 0.131066 (16:41:41)\n",
      "Finished training step 3072/1024218 of epoch 0, 13.12 ms/step, loss 0.130116 (16:41:54)\n",
      "Finished training step 4096/1024218 of epoch 0, 13.02 ms/step, loss 0.129073 (16:42:07)\n",
      "Finished training step 5120/1024218 of epoch 0, 13.39 ms/step, loss 0.129285 (16:42:21)\n",
      "Finished training step 6144/1024218 of epoch 0, 12.91 ms/step, loss 0.128849 (16:42:34)\n",
      "Finished training step 7168/1024218 of epoch 0, 12.69 ms/step, loss 0.128080 (16:42:47)\n",
      "Finished training step 8192/1024218 of epoch 0, 12.93 ms/step, loss 0.128248 (16:43:01)\n",
      "Finished training step 9216/1024218 of epoch 0, 12.93 ms/step, loss 0.127882 (16:43:14)\n",
      "Finished training step 10000/1024218 of epoch 0, 12.27 ms/step, loss 0.128126 (16:43:23)\n",
      "Testing at step 10000/1024218 of epoch 0\n",
      "Testing on 89128960 elements\n",
      " AUC 0.7727, best AUC 0.7727, validation_time (on IPU) 22.882 s, validation_time (total) 24.428 s\n"
     ]
    }
   ],
   "source": [
    "# uncomment the following lines to run training (it requires having downloaded the train dataset)\n",
    "# ht_experiment.run_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting a maximum number of rows per embedding table equal to `max_ind_range = 5000`, the resulting number of embedding parameters is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10255488\n"
     ]
    }
   ],
   "source": [
    "print(ht_experiment.model.n_emb_parameters)\n",
    "ht_experiment.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corresponds to a **compression factor of ~2500x** compared to the MLPerf reference configuration, passing from 104 GB of embedding tables to a mere 41 MB, which can easily fit in the SRAM of one IPU. But is this model still able to reach the quality target of 0.8025 AUROC? Let us compare its validation curve with the one of the uncompressed model (run on 8 A100 GPUs), when using the same training regime (SGD optimizer, learning rate = 1.0, global batch size = 2048) as in the reference implementation.\n",
    "\n",
    "\n",
    "<img src=\"img/UNCvsHT.png\" height=\"400\">\n",
    "\n",
    "As we see, despite the large compression the model using the Hashing Trick is still able to reach the quality target. However, **the convergence speed has degraded**: the compressed model takes 4.4 epochs to reach the threshold, while the uncompressed model only takes one. It is important to notice, however, the the reduced parameter count of the compressed model removes the need for model parallelism, lowering the communication costs for embedding lookups and thus the wall time per iteration. When distributing over just 4 Bow IPUs, the training throughput is **2.5 times higher** than what we observe for the uncompressed model deployed on 8 A100 GPUs (trained with the PyTorch implementation in Meta's repository).\n",
    "\n",
    "To load the final checkpoint of the compressed model and validate on the test dataset, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data file: /net/group/research/criteo_datasets/criteo_terabyte/terabyte_processed_test.bin number of batches: 21762\n",
      "Loading saved model checkpoints/ht_sgd.pt\n",
      "Saved at: epoch = 5/5, step = 1024000/1024218\n",
      "Training state: loss = 0.12198\n",
      "Testing state: AUC = 0.80312\n",
      "Compiling inference model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:29:45.534] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 563\n",
      "Graph compilation: 100%|██████████| 100/100 [00:44<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on 89137152 elements using 4 IPUs\n",
      " AUC 0.8031, best AUC 0.8031, validation_time (on IPU) 44.047 s, validation_time (total) 45.615 s\n"
     ]
    }
   ],
   "source": [
    "ht_exec_config.load_model = checkpoint_dir.joinpath(\"ht_sgd.pt\")\n",
    "ht_inf_ex = Experiment(\n",
    "    dlrm_config=ht_dlrm_config,\n",
    "    exec_config=ht_exec_config,\n",
    ")\n",
    "ht_inf_ex.run_inference()\n",
    "ht_inf_ex.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compositional Embedding and DHE\n",
    "\n",
    "We now test Compositional Embedding and DHE to see whether they perform better than the basic Hashing Trick. We will choose configurations for the two embedding compression techniques so that they still achieve a compression factor of ~2500x compared to the original model. Hyperparameters are then tuned via sweeps to achieve best performance.\n",
    "\n",
    "For each model we specify the configuration used for training and provide the final checkpoint to perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data file: /net/group/research/criteo_datasets/criteo_terabyte/terabyte_processed_test.bin number of batches: 21762\n",
      "Compositional Embedding: dim:128 weight_size:torch.Size([312500, 32]) chunk_size:32\n",
      "Loading saved model checkpoints/compemb_sgd.pt\n",
      "Saved at: epoch = 4/4, step = 1024000/1024218\n",
      "Training state: loss = 0.12192\n",
      "Testing state: AUC = 0.80332\n",
      "Compiling inference model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:31:39.988] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 710\n",
      "[16:31:39.989] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 713\n",
      "[16:31:39.997] [poptorch:cpp] [warning] [DISPATCHER] Tensor (ptr 0xa4525f0) type coerced from Long to Int\n",
      "Graph compilation: 100%|██████████| 100/100 [01:08<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# embedding parameters: 10000000\n",
      "\n",
      "Testing on 89137152 elements using 4 IPUs\n",
      " AUC 0.8033, best AUC 0.8033, validation_time (on IPU) 43.019 s, validation_time (total) 44.548 s\n"
     ]
    }
   ],
   "source": [
    "# Compositional Embedding\n",
    "\n",
    "compemb_ex_config = ExecutionConfig(\n",
    "    data_path=dataset_dir,\n",
    "    ipu_replicas_training=8,\n",
    "    ipu_replicas_inference=NUM_AVAILABLE_IPUS,\n",
    "    print_freq=1024,\n",
    "    test_freq=204800,\n",
    "    load_model=checkpoint_dir.joinpath(\"compemb_sgd.pt\")\n",
    "    # wandb_logging=True,\n",
    ")\n",
    "compemb_tr_config = TrainingConfig(\n",
    "    mini_batch_size=512,\n",
    "    optimizer=\"sgd\",\n",
    "    learning_rate=2.0,\n",
    ")\n",
    "compemb_dlrm_config = DLRMConfig(\n",
    "    embedding_size,\n",
    "    bottom_mlp_arch,\n",
    "    top_mlp_arch,\n",
    "    compression=CompressionConfig(\n",
    "        \"comp_emb\", comp_emb_chunk_size=32, comp_emb_n_parameters=10000000\n",
    "    ),  # chunks of size 32, thus concatenate 4 chunks per embedding\n",
    ")\n",
    "\n",
    "compemb_ex = ex = Experiment(\n",
    "    compemb_dlrm_config,\n",
    "    compemb_ex_config,\n",
    "    # compemb_tr_config,\n",
    ")\n",
    "\n",
    "print(f\"\\n# embedding parameters: {compemb_ex.model.n_emb_parameters}\\n\")\n",
    "compemb_ex.run_inference()\n",
    "compemb_ex.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data file: /net/group/research/criteo_datasets/criteo_terabyte/terabyte_processed_test.bin number of batches: 21762\n",
      "DHE: dim:128 n_hash_per_table:512 mlp_dims:[512 512 256 128]\n",
      "Loading saved model checkpoints/dhe_sgd.pt\n",
      "Saved at: epoch = 4/4, step = 1024000/1024218\n",
      "Training state: loss = 0.12225\n",
      "Testing state: AUC = 0.80329\n",
      "Compiling inference model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:34:27.190] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 795\n",
      "[16:34:27.199] [poptorch:cpp] [warning] [DISPATCHER] Tensor (ptr 0x17a9fc90) type coerced from Long to Int\n",
      "Graph compilation: 100%|██████████| 100/100 [00:40<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# embedding parameters: 11098880\n",
      "\n",
      "Testing on 89137152 elements using 4 IPUs\n",
      " AUC 0.8033, best AUC 0.8033, validation_time (on IPU) 50.647 s, validation_time (total) 52.178 s\n"
     ]
    }
   ],
   "source": [
    "# DHE\n",
    "\n",
    "dhe_ex_config = ExecutionConfig(\n",
    "    data_path=dataset_dir,\n",
    "    ipu_replicas_training=16,\n",
    "    ipu_replicas_inference=NUM_AVAILABLE_IPUS,\n",
    "    grad_accum=2,\n",
    "    print_freq=1024,\n",
    "    test_freq=204800,\n",
    "    load_model=checkpoint_dir.joinpath(\"dhe_sgd.pt\")\n",
    "    # wandb_logging=True,\n",
    ")\n",
    "dhe_tr_config = TrainingConfig(\n",
    "    mini_batch_size=128,\n",
    "    optimizer=\"sgd\",\n",
    "    learning_rate=0.75,\n",
    ")\n",
    "dhe_dlrm_config = DLRMConfig(\n",
    "    embedding_size,\n",
    "    bottom_mlp_arch,\n",
    "    top_mlp_arch,\n",
    "    compression=CompressionConfig(\"dhe\", dhe_n_hash=512, dhe_mlp_dims=\"512-256-128\"),\n",
    ")\n",
    "\n",
    "dhe_ex = ex = Experiment(\n",
    "    dhe_dlrm_config,\n",
    "    dhe_ex_config,\n",
    "    # dhe_tr_config,\n",
    ")\n",
    "\n",
    "print(f\"\\n# embedding parameters: {dhe_ex.model.n_emb_parameters}\\n\")\n",
    "dhe_ex.run_inference()\n",
    "dhe_ex.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the evolution of validation AUROC during training:\n",
    "\n",
    "<img src=\"img/compression_comparison.png\" height=\"400\">\n",
    "\n",
    "**Compositional Embedding and DHE both outperform the Hashing Trick**, reaching the MLPerf quality target in 2.6 and 3 epochs respectively and then steadily continuing to improve past it.\n",
    "\n",
    "As a general rule, it is important to notice that **the number of training steps required for the model to reach a target performance will increase as the compression rate increases** (assuming, of course, that the compressed model can still achieve that performance level). As an example, **using Compositional Embedding we can push the compression factor to 10,000x** (as in [[Desai *et al.*, 2022](https://proceedings.neurips.cc/paper_files/paper/2022/hash/dbae915128892556134f1c5375855590-Abstract-Conference.html)]), with trainable parameters used to generate embeddings only taking up 10 MB (`comp_emb_n_parameters=2700000` in `CompressionConfig`). However, the MLPerf quality target is now reached only after 4.6 epochs of training.\n",
    "\n",
    "<img src=\"img/compemb_comparison.png\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "As previously stated, one potential advantage of the reduced memory footprint of compressed models is the possibility to use more advanced optimizers than SGD. To test their effect, we conducted extensive experiments training the compressed model with Adam, using both Compositional Embedding and DHE. This proved to be quite challenging when using Adam for all the model's parameters, often incurring in training instability and with results being very sensitive on the choice of hyperparameters (for DHE, similar challenges were reported in [[Kang *et al.*, 2021](https://dl.acm.org/doi/10.1145/3447548.3467304)]). We therefore resorted to use a **hybrid optimizer** (similar to the one adopted in the MLPerf implementation described [here](https://sigopt.com/blog/optimize-the-deep-learning-recommendation-model-with-intelligent-experimentation/)), with Adam applied to only the bottom and top MLP and SGD still being used for the embedding parameters. This leads to a significant improvement in both model quality and convergence speed for Compositional Embedding, with our **best model now able to reach the MLPerf threshold of 0.8025 AUROC in just one epoch**, as the baseline uncompressed model.\n",
    "\n",
    "<img src=\"img/compemb_adam.png\" height=\"400\">\n",
    "\n",
    "This is the configuration of the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data file: /net/group/research/criteo_datasets/criteo_terabyte/terabyte_processed_test.bin number of batches: 21762\n",
      "Compositional Embedding: dim:128 weight_size:torch.Size([312500, 32]) chunk_size:32\n",
      "Loading saved model checkpoints/compemb_adamsgd.pt\n",
      "Saved at: epoch = 4/4, step = 512109/512109\n",
      "Training state: loss = 0.12168\n",
      "Testing state: AUC = 0.80391\n",
      "Compiling inference model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:36:06.288] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 890\n",
      "[16:36:06.289] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 893\n",
      "[16:36:06.297] [poptorch:cpp] [warning] [DISPATCHER] Tensor (ptr 0xa43e580) type coerced from Long to Int\n",
      "Graph compilation: 100%|██████████| 100/100 [01:07<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# embedding parameters: 10000000\n",
      "\n",
      "Testing on 89137152 elements using 4 IPUs\n",
      " AUC 0.8039, best AUC 0.8039, validation_time (on IPU) 44.739 s, validation_time (total) 46.322 s\n"
     ]
    }
   ],
   "source": [
    "# Compositional Embedding (Adam+SGD)\n",
    "\n",
    "compemb_ex_config = ExecutionConfig(\n",
    "    data_path=dataset_dir,\n",
    "    ipu_replicas_training=16,\n",
    "    ipu_replicas_inference=NUM_AVAILABLE_IPUS,\n",
    "    print_freq=512,\n",
    "    test_freq=102400,\n",
    "    load_model=checkpoint_dir.joinpath(\"compemb_adamsgd.pt\")\n",
    "    # wandb_logging=True,\n",
    ")\n",
    "compemb_tr_config = TrainingConfig(\n",
    "    mini_batch_size=512,\n",
    "    optimizer=\"adam-sgd\",\n",
    "    learning_rate=0.0001,\n",
    "    emb_learning_rate=4.8,\n",
    "    beta1=0.87,\n",
    "    lr_scheduler=\"warmup-decay\",\n",
    "    lr_num_warmup_steps=0.15,\n",
    "    lr_decay_start_step=0.28,\n",
    "    lr_num_decay_steps=0.85,\n",
    "    lr_min=0.005,\n",
    "    schedule_emb_lr=False,\n",
    ")\n",
    "compemb_dlrm_config = DLRMConfig(\n",
    "    embedding_size,\n",
    "    bottom_mlp_arch,\n",
    "    top_mlp_arch,\n",
    "    compression=CompressionConfig(\n",
    "        \"comp_emb\", comp_emb_chunk_size=32, comp_emb_n_parameters=10000000\n",
    "    ),  # chunks of size 32, thus concatenate 4 chunks per embedding\n",
    ")\n",
    "\n",
    "compemb_ex = ex = Experiment(\n",
    "    compemb_dlrm_config,\n",
    "    compemb_ex_config,\n",
    "    # compemb_tr_config,\n",
    ")\n",
    "\n",
    "print(f\"\\n# embedding parameters: {compemb_ex.model.n_emb_parameters}\\n\")\n",
    "compemb_ex.run_inference()\n",
    "compemb_ex.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly enough, despite extensive hyperparameter sweeping we weren't able to improve DHE convergence speed when applying a similar hybrid optimization scheme, with models failing to match the final performance of the ones trained with SGD only (and struggling even to reach the MLPerf quality target in a similar number of steps).\n",
    "\n",
    "<img src=\"img/dhe_adam.png\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Based on evidence the DLRM-style models are already operating in an over-parametrized regime [[Ardalani *et al.*, 2022](https://arxiv.org/abs/2208.08489)], in this notebook we compared several ways to reduce their memory footprint, focusing in particular on embedding operators — which are typically responsible for the vast majority of model's parameters. This can be achieved by sharing embeddings between several categorical items, or — to lower the likelihood of collisions — by replacing embedding operators with modules generating embeddings on-the-fly. Experimenting on the recommendation task of MLPerf, we found that the quality target could still be reached even when reducing the number of parameters allocated to embeddings by 10,000x. This comes with a caveat, namely the need to train for a larger number of iterations as the compression factor increases, in order to achieve the same saturation. However, as the compressed model can now fit in the memory of a single device and does not require model-parallelism anymore (thus saving on expensive embedding lookups across devices), training and inference iterations become cheaper, arguably tilting the cost-benefit balance in favour of compression. Moreover, the convergence speed can potentially be boosted by improving the optimization scheme, taking advantage of the fewer constraints imposed on this by parameter count.\n",
    "\n",
    "Compressed DLRM architectures find a very good match in IPUs, where the large, high-bandwidth on-chip SRAM can accommodate all parameters of the compressed model even for less extreme compression rates than the ones tested here, making possible to explore a wide range of configurations to find the best trade-off between memory footprint and training speed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
